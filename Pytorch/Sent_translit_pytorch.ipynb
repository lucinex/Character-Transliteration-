{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sent_translit_pytorch.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1XUUzB9fwJKAv80mdyk77k5seXN9a4zww","authorship_tag":"ABX9TyPx4YpBscGqxaiyVs1Gt4UC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"78e96dd5a54e4653906c4ad52623dd8e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b1775ebcfc5740ffbf2b614ace704276","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_66d1d9fa9dc041e9ba99335b5894c517","IPY_MODEL_16dcf11054a247448c4e43f70d7869b0","IPY_MODEL_71a1c1eacb874027b3f1ef66118310fe"]}},"b1775ebcfc5740ffbf2b614ace704276":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"66d1d9fa9dc041e9ba99335b5894c517":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7b98eefd9b704bc7b53b9854a94442b9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 52%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cc97f5dbcd67451ba084d9784f55cabe"}},"16dcf11054a247448c4e43f70d7869b0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3abb5bd435b24978ac5200f0becfdb07","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":138,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":72,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_decc780c8f424951bf0d81633f39fabf"}},"71a1c1eacb874027b3f1ef66118310fe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_321c139f2cc0432895a1d11f167e952e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 72/138 [01:06&lt;00:59,  1.12it/s, loss=29.9]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a19adf5f4be6460d9e012d5b2816b22a"}},"7b98eefd9b704bc7b53b9854a94442b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cc97f5dbcd67451ba084d9784f55cabe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3abb5bd435b24978ac5200f0becfdb07":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"decc780c8f424951bf0d81633f39fabf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"321c139f2cc0432895a1d11f167e952e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a19adf5f4be6460d9e012d5b2816b22a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9b682875356a455a8db61276b834ad02":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_04f6ec7e1fe841dda2ddc51fce330a92","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7015ad1ad3c2421685abbd9f38b1ae66","IPY_MODEL_a87cf3eccb254b9a93b98441da55fecb","IPY_MODEL_9036d52d841d4081a3ae2f0d83f9ace0"]}},"04f6ec7e1fe841dda2ddc51fce330a92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7015ad1ad3c2421685abbd9f38b1ae66":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6793b2d937494b2d8ba3fd7b249abb07","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2d27342055b64878a0eb488b0ad525d4"}},"a87cf3eccb254b9a93b98441da55fecb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_17451388bc1448fb9d5e043e945f1904","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":16,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":16,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d03856e9bee6421da9ee3f2eb77bb8d5"}},"9036d52d841d4081a3ae2f0d83f9ace0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b702e448426c44e099703f97b74c961b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 16/16 [00:06&lt;00:00,  2.43it/s, loss=25.1]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4f63dbf3eff94967a317f9a3215fb065"}},"6793b2d937494b2d8ba3fd7b249abb07":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2d27342055b64878a0eb488b0ad525d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"17451388bc1448fb9d5e043e945f1904":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d03856e9bee6421da9ee3f2eb77bb8d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b702e448426c44e099703f97b74c961b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4f63dbf3eff94967a317f9a3215fb065":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2322172f049a4640a41e2a485e94f210":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d47c6eb6d4054fa2b7e43a31d69f5d19","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_80a7cdbd1b6448e99a4b91b938e5dbd7","IPY_MODEL_714bb83d8f064cc2b65961f12b0216ba","IPY_MODEL_c99ed233581a40b98bd2f69385646663"]}},"d47c6eb6d4054fa2b7e43a31d69f5d19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"80a7cdbd1b6448e99a4b91b938e5dbd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1768925ed49f4aefa1d3fec7fdd8921b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"  8%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_879535abb8864238adfbccc402f75d39"}},"714bb83d8f064cc2b65961f12b0216ba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9f2c768b27ba46bf880c217beb81ec43","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"danger","max":138,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":11,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3be64572218c452aa880fde3362f75ae"}},"c99ed233581a40b98bd2f69385646663":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2a9c16b22e304f0c8ea7709c5f9b9114","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 11/138 [00:10&lt;01:50,  1.15it/s, loss=30.1]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_75764f14953f45728ddd93c5af8345df"}},"1768925ed49f4aefa1d3fec7fdd8921b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"879535abb8864238adfbccc402f75d39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9f2c768b27ba46bf880c217beb81ec43":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3be64572218c452aa880fde3362f75ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2a9c16b22e304f0c8ea7709c5f9b9114":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"75764f14953f45728ddd93c5af8345df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3GBc3GBYPTHt","executionInfo":{"status":"ok","timestamp":1630514800909,"user_tz":-330,"elapsed":6432,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"32ddc333-2e21-4dab-c9ea-0569adcca0b6"},"source":["#!pip install -U torchtext==0.8.0\n","\n","from torchtext.data import field,BucketIterator, TabularDataset\n","!pip install torchtext --upgrade"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.8.0)\n","Collecting torchtext\n","  Using cached torchtext-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n","Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.9.0+cu102)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.62.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext) (3.7.4.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n","Installing collected packages: torchtext\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.8.0\n","    Uninstalling torchtext-0.8.0:\n","      Successfully uninstalled torchtext-0.8.0\n","Successfully installed torchtext-0.10.0\n"]}]},{"cell_type":"code","metadata":{"id":"SkGFcEwbuzLR"},"source":["\n","\n","import csv \n","import os \n","\n","import torch \n","import torchtext\n","import torch.nn.functional as F\n","\n","from torchtext.data import get_tokenizer\n","from torch.utils.data import Dataset, DataLoader\n","from torchtext.data.utils import ngrams_iterator\n","from torchtext.data.metrics import bleu_score\n","\n","import torch.nn as nn\n","from torch.autograd import Variable\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","import itertools\n","import string\n","\n","import pandas as pd\n","import numpy as np\n","import multiprocessing\n","import os, io, json\n","import random\n","\n","import warnings\n","from tqdm.notebook import tqdm\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SOYaWAo2u2cJ","executionInfo":{"status":"ok","timestamp":1630514800911,"user_tz":-330,"elapsed":19,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"e51f6fdd-856c-4a57-c484-ea9864abb61c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"rAHXihq5vFC2"},"source":["path_1 = \"/content/drive/MyDrive/Dataset/Dakshina/hi/hi.romanized.rejoined.dev.native.txt\"\n","path_2 = \"/content/drive/MyDrive/Dataset/Dakshina/hi/hi.romanized.rejoined.dev.roman.txt\"\n","path_3 = \"/content/drive/MyDrive/Dataset/Dakshina/hi/hindi_joined.csv\"\n","pth_w2v_1 = \"/content/drive/MyDrive/Dataset/Dakshina/hi/wordembed/w2v_native_1.model\"\n","pth_w2v_r_2 = \"/content/drive/MyDrive/Dataset/Dakshina/hi/wordembed/w2v_roman_1.model\"\n","\n","pth_ft_1 = \"/content/drive/MyDrive/Dataset/Dakshina/hi/wordembed/fastText_native_1.model\"\n","path_embedw2v_150_native = \"/content/drive/MyDrive/Dataset/Dakshina/hi/wordembed/native_w2v_embed.pickle\"\n","path_tokenizer_native = \"/content/drive/MyDrive/Dataset/Dakshina/hi/tokenizer+/native_train_tokenizer.json\"\n","path_xtrain_native = \"/content/drive/MyDrive/Dataset/Dakshina/hi/xtrain_hi.pickle\"\n","\n","path_embedw2v_150_roman = \"/content/drive/MyDrive/Dataset/Dakshina/hi/wordembed/roman_w2v_embed.pickle\"\n","path_tokenizer_roman = \"/content/drive/MyDrive/Dataset/Dakshina/hi/tokenizer+/roman_train_tokenizer.json\"\n","path_ytrain_roman = \"/content/drive/MyDrive/Dataset/Dakshina/hi/ytrain_hi.pickle\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sujShIKYpKdI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XDKzRsNfjz-q"},"source":["# for reproducibility\n","# refer https://pytorch.org/docs/stable/notes/randomness.html\n","SEED = 42\n","\n","\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mD3zPT-DvLmT"},"source":["native = []\n","with open(path_1,'r')as fp:\n","   lines= fp.read().split(\"\\n\")\n","   for line in lines:\n","     native.append(line)\n","native"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fwkjldn_vewu"},"source":["roman = []\n","with open(path_2,'r')as fp:\n","   lines= fp.read().split(\"\\n\")\n","   for line in lines:\n","     roman.append(line)\n","roman"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gzHny0-vxisX"},"source":["df1 = pd.DataFrame(native,columns=[\"Hindi\"])\n","df2 = pd.DataFrame(roman)\n","df1[\"roman\"] = df2\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2NqgYUe50enB","executionInfo":{"status":"ok","timestamp":1629971883341,"user_tz":-330,"elapsed":716,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"2c9f1346-3508-4048-9019-6c1be0678577"},"source":["count_max = 0 \n","\n","lines = 0\n","for i,line in enumerate(native):\n","  count= 0\n","  line = line.split(\" \")\n","  if len(line)<50:\n","    lines+=1\n","print(count_max,lines) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 4880\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TPmWqbcXv4St","executionInfo":{"status":"ok","timestamp":1629969854956,"user_tz":-330,"elapsed":763,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"fe89e6c7-5e13-433f-d269-6a62bdcc5a2c"},"source":["print(len(native),len(roman))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["5001 5001\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uWHj-AINxzve"},"source":["df1.to_csv(path_3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o_t6ZYxXx7SN"},"source":["df = pd.read_csv(path_3,index_col=[0])\n","df.dropna(inplace=True,axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ArB0zq0ypQu","executionInfo":{"status":"ok","timestamp":1629991152393,"user_tz":-330,"elapsed":674,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"f364e947-42fd-4264-dc03-9c939a5fd46f"},"source":["df2 = df.copy()\n","for i,line in enumerate(df2[\"Hindi\"]):\n","  line = line.split(\" \")\n","  if len(line) > 50:\n","    df2.drop(index=i,axis=0,inplace = True)\n","len(df2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4888"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"TAFvcue59lJr"},"source":["df2.to_csv(path_3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cBbxQ9OJp3cx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AIXhz7Ocp3ff"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T7iMvZPYp3i_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_YlXi1y8C2jg","executionInfo":{"status":"error","timestamp":1631248945098,"user_tz":-330,"elapsed":791,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"4ca74c8e-b4d7-4431-b9f3-bd661ca9c5ae","colab":{"base_uri":"https://localhost:8080/","height":167}},"source":["df2 = pd.read_csv(path_3,index_col=[0])"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-563cfc025352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}]},{"cell_type":"code","metadata":{"id":"YAQyj1QFyuIu"},"source":["hindi = df2[\"Hindi\"]\n","roman = df2[\"roman\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wEg7wqz7QAg0"},"source":["from sklearn.model_selection import train_test_split\n","\n","train , val = train_test_split(df2[[\"Hindi\",\"roman\"]], test_size=0.1)\n","train.to_csv(\"train.csv\",index=None)\n","val.to_csv(\"val.csv\",index=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYWeJaQLN8Xk"},"source":["Field = field.Field"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"awFkY1BMQAjp"},"source":["tokenizer = get_tokenizer(None) \n","\n","\n","def tokenize_(sentence):\n","  return [tokenizer(words) for words in sentence]\n","\n","hi_text = Field(tokenize=tokenizer,use_vocab=True)\n","en_text = Field(tokenize=tokenizer,use_vocab=True,eos_token=\"<eos>\",init_token=\"<sos>\",is_target=True) \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vSoQM6bUmCm3","executionInfo":{"status":"ok","timestamp":1630504017835,"user_tz":-330,"elapsed":537,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"8fa7c6aa-6e5b-4623-988a-cd725cc06bed"},"source":[""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['usne',\n"," 'Vindhayvasini',\n"," 'Devi',\n"," 'ki',\n"," 'puja',\n"," 'ke',\n"," 'liye',\n"," 'prerit',\n"," 'kiya.']"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"htp_qyZLQAmM"},"source":["data_fields = [('Hindi', hi_text), ('roman', en_text)]\n","train= TabularDataset(path = \"/content/train.csv\",\n","                            format='csv', skip_header=True,fields=data_fields)\n","val= TabularDataset(path = \"/content/val.csv\",\n","                            format='csv', skip_header=True,fields=data_fields)\n","\n","hi_text.build_vocab(train.Hindi,val.Hindi)\n","en_text.build_vocab(train.roman,val.roman)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"FPU904TUtWGo","executionInfo":{"status":"ok","timestamp":1630514819391,"user_tz":-330,"elapsed":12,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"434eb63a-dc9a-47e7-9b24-196af5834883"},"source":["device = ('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'cpu'"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"7fR-1V4jtWJl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ca4v3qYlzyqX"},"source":["train_dataloader,val_dataloader = torchtext.data.BucketIterator.splits(\n","    \n","                              # Datasets for iterator to draw data from\n","                              (train,val),\n","\n","                              # Tuple of train and validation batch sizes.\n","                              batch_sizes=(32,32),\n","\n","                              # Device to load batches on.\n","                              device=device, \n","\n","                              # Function to use for sorting examples.\n","                              sort_key=lambda x: len(x.Hindi),\n","\n","\n","                              # Repeat the iterator for multiple epochs.\n","                              repeat=False, \n","\n","                              # Sort all examples in data using `sort_key`.\n","                              sort=False, \n","\n","                              # Shuffle data on each epoch run.\n","                              shuffle=True,\n","\n","                              # Use `sort_key` to sort examples in each batch.\n","                              sort_within_batch = False,\n","                              \n","                              )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ash98YDotWSY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x0i2KI-na5hX"},"source":["class Encoder(nn.Module):\n","    \"\"\" Encoder encodes the input sequence into a dense embedded context.\n","    \"\"\"\n","    \n","    def __init__(self, input_size, embed_size, hidden_size, n_layers=1, dropout_rate=0.5):\n","        super(Encoder, self).__init__()\n","        self.input_size = input_size\n","        self.embed_size = embed_size\n","        self.hidden_size = hidden_size\n","        self.n_layers = n_layers\n","        self.dropout_rate = dropout_rate\n","        self.embedding = nn.Embedding(input_size, embed_size)\n","        self.rnn = nn.GRU(embed_size, hidden_size, n_layers, bidirectional=True, dropout=dropout_rate)\n","        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n","        self.dropout = nn.Dropout(dropout_rate)\n","    \n","\n","    def load_pretrained_embeddings(self, embeddings, fine_tune=False):\n","        self.embedding.weight = nn.Parameter(embeddings)\n","        for p in self.embedding.parameters():\n","            p.requires_grad = fine_tune\n","    \n","    \n","    def forward(self, x, lengths):\n","        \"\"\" x: (seq_len, batch_size)\n","            lengths: (batch_size)\n","        \"\"\"\n","        lengths, perm_idx = lengths.sort(0, descending=True)\n","        \n","        x = x.T[perm_idx]\n","        x = x.T\n","\n","        embed = self.dropout(self.embedding(x))  # (seq_len, batch_size, embed_size)\n","        \n","        # packing the embedding sequence to avoid unnecessary computations\n","        # refer https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n","        packed_embed = pack_padded_sequence(embed, lengths.cpu().numpy())\n","        packed_outputs, hidden = self.rnn(packed_embed)\n","        outputs, _ = pad_packed_sequence(packed_outputs)\n","        \n","        # outputs is a non-packed sequence of all hidden states from last layer\n","        # outputs: (seq_len, batch_size, hidden_size * 2)\n","        # hidden is the final non-padded time stepped state\n","        # hidden: (n_layers * 2, batch_size, hidden_size)\n","        \n","        # initial decoder hidden state\n","        hidden = F.relu(self.fc(torch.cat([h for h in hidden[-2:]], dim=1)))  # (batch_size, hidden_size)\n","        print(output.shape)\n","        print(hidden.shape)\n","        # outputs: (seq_len, batch_size, hidden_size * 2)\n","        # hidden: (batch_size, hidden_size)\n","        return outputs, hidden\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-11WY9S8kSg7"},"source":["class BahdanauAttention(nn.Module):\n","    \n","    \n","    def __init__(self, hidden_size):\n","        super(BahdanauAttention, self).__init__()\n","        self.hidden_size = hidden_size\n","        \n","        self.W1 = nn.Linear(hidden_size * 2, hidden_size, bias=False)\n","        self.W2 = nn.Linear(hidden_size, hidden_size, bias=False)\n","        self.V = nn.Linear(hidden_size, 1, bias=False)\n","    \n","    \n","    def forward(self, hidden, enc_outputs, enc_masks):\n","        \"\"\" hidden: (batch_size, hidden_size)\n","            enc_outputs: (seq_len, batch_size, hidden_size * 2)\n","            enc_masks: (batch_size, seq_len)\n","        \"\"\"\n","        seq_len = enc_outputs.shape[0]\n","        \n","        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)  # (batch_size, seq_len, hidden_size)\n","        enc_outputs = enc_outputs.permute(1, 0, 2)  # (batch_size, seq_len, hidden_size * 2)\n","        \n","        # calculate alignment scores\n","        energy = F.relu(self.W1(enc_outputs) + self.W2(hidden))  # (batch_size, seq_len, hidden_size)\n","        scores = self.V(energy)  # (batch_size, seq_len, 1)\n","        scores = scores.squeeze(2)  # (batch_size, seq_len)\n","        \n","        # mask out invalid positions\n","        scores = scores.masked_fill(enc_masks == 0, float('-inf'))  # (batch_size, seq_len)\n","        \n","        # calculate the attention weights (prob) from alignment scores\n","        alphas = F.softmax(scores, dim=-1)  # (batch_size, seq_len)\n","        \n","        # calculate context vector\n","        context_vector = torch.bmm(alphas.unsqueeze(1), enc_outputs)  # (batch_size, 1, hidden_size * 2)\n","        \n","        # context_vector: (batch_size, 1, hidden_size * 2)\n","        # alphas: (batch_size, seq_len)\n","        return context_vector, alphas"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oc_BHE1o8FT0"},"source":["class DecoderWithAttention(nn.Module):\n","    \"\"\" decode the encoded dense embedding context into a sequence \n","        with use of attention mechanism.\n","    \"\"\"\n","    def __init__(self, output_size, embed_size, hidden_size, n_layers=1, dropout_rate=0.5):\n","        super(DecoderWithAttention, self).__init__()\n","        self.output_size = output_size\n","        self.embed_size = embed_size\n","        self.hidden_size = hidden_size\n","        self.n_layers = n_layers\n","        self.dropout_rate = dropout_rate\n","        \n","        self.embedding = nn.Embedding(output_size, embed_size)\n","        self.rnn = nn.GRU(embed_size + hidden_size * 2, hidden_size, n_layers, bidirectional=False, dropout=0.6)\n","        self.attention = BahdanauAttention(hidden_size)\n","        self.fc1 = nn.Linear(embed_size + hidden_size + hidden_size * 2, output_size) #output_size\n","        \n","        self.dropout = nn.Dropout(0.4)\n","        self.tanh = nn.Tanh()\n","        self.relu = nn.ReLU()\n","\n","        \n","        \n","    \n","    def load_pretrained_embeddings(self, embeddings, fine_tune=False):\n","        self.embedding.weight = nn.Parameter(embeddings)\n","        for p in self.embedding.parameters():\n","            p.requires_grad = fine_tune\n","    \n","    \n","    def forward(self, x, hidden, enc_outputs, enc_masks):\n","        \"\"\" x: (batch_size)\n","            hidden: (batch_size, hidden_size)\n","            enc_outputs: (seq_len, batch_size, hidden_size * 2)\n","            enc_masks: (batch_size, seq_len)\n","        \"\"\"\n","        embed = self.dropout(self.embedding(x.unsqueeze(0)))  # (1, batch_size, embed_size)\n","        \n","        # calculate attention weights and context vector\n","        context_vector, alphas = self.attention(hidden, enc_outputs, enc_masks)\n","        \n","        # context_vector: (batch_size, 1, hidden_size * 2)\n","        # alphas: (batch_size, seq_len)\n","        \n","        context_vector = context_vector.permute(1, 0, 2)  # (1, batch_size, hidden_size * 2)\n","        \n","        # input to the rnn is a concatenation of embedding and weighted encoder states (context)\n","        rnn_input = torch.cat((embed, context_vector), dim=2)  # (1, batch_size, embed_size + hidden_size * 2)\n","        \n","        # update the rnn hidden state\n","        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n","        \n","        # output: (seq_len=1, batch_size, hidden_size)\n","        # hidden: (n_layers=1, batch_size, hidden_size)\n","        # seq_len and n_layers are always equal to 1 for this decoder\n","        \n","        assert (output == hidden).all()\n","        \n","        embed = embed.squeeze(0)  # (batch_size, embed_size)\n","        output = output.squeeze(0)  # (batch_size, hidden_size)\n","        context_vector = context_vector.squeeze(0)  # (batch_size, hidden_size * 2)\n","        \n","        prediction = self.dropout(self.fc1(torch.cat((embed, output, context_vector), dim=1)))  # (batch_size, output_size)\n","        \n","        \n","        hidden = hidden.squeeze(0)  # (batch_size, hidden_size)\n","        \n","        # prediction: (batch_size, output_size)\n","        # hidden: (batch_size, hidden_size)\n","        # alphas: (batch_size, seq_len)\n","        return prediction, hidden, alphas\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pkvQj9hMb-dJ"},"source":["import random\n","class Seq2SeqWithAttention(nn.Module):\n","    \"\"\" Seq2Seq wrapper for encoder and decoder.\n","    \"\"\"\n","    \n","    def __init__(self, encoder, decoder, pad_idx):\n","        super(Seq2SeqWithAttention, self).__init__()\n","        self.pad_idx = pad_idx\n","        self.device = (\"cuda\" if torch.cuda.is_available() else 'cpu')\n","        self.encoder = encoder\n","        self.decoder = decoder\n","    \n","    \n","    def forward(self, source, target, teacher_force_ratio=0.5):\n","        \"\"\" source: (source_seq_len, batch_size)\n","            source_lengths: (batch_size)\n","            target: (target_seq_len, batch_size)\n","        \"\"\"\n","        source_lengths =torch.LongTensor(list(map(self.length, source.T)))\n","        \n","        source_lengths = source_lengths.to(source.device)\n","        target_seq_len, batch_size = target.shape\n","        target_vocab_size = self.decoder.output_size\n","        \n","        # initialize tensor to store decoder outputs\n","        outputs = torch.zeros((target_seq_len, batch_size, target_vocab_size)).to(source.device)\n","        \n","        # outputs: (target_seq_len, batch_size, output_size)\n","        \n","        # encode the source sequence\n","        enc_outputs, hidden = self.encoder(source,source_lengths)\n","        \n","        # enc_outputs: (seq_len, batch_size, hidden_size * 2)\n","        # hidden: (batch_size, hidden_size)\n","        \n","        # create mask for source sequence\n","        enc_masks = (source != self.pad_idx).permute(1, 0)  # (batch_size, seq_len)\n","        \n","        # initial input to the decoder is the SOS_TOKEN\n","        x = target[0, :]  # (batch_size)\n","        \n","        for t in range(1, target_seq_len):\n","            # decode the encoded sequence for current time step\n","            output, hidden, _ = self.decoder(x, hidden, enc_outputs, enc_masks)\n","            \n","            # output: (batch_size, output_size)\n","            # hidden: (batch_size, hidden_size)\n","            \n","            # store the decoder outputs for current time step\n","            outputs[t] = output\n","            \n","            # get the highest predicted token (best word) for current time step\n","            top1 = output.argmax(dim=1)\n","           \n","            x = target[t] if random.random() < teacher_force_ratio else top1\n","        \n","        # outputs: (target_seq_len, batch_size, output_size)\n","        return outputs\n","    def length(self,vector):\n","        new_vec = []\n","        for vec in vector:\n","          if not vec==1:\n","              new_vec.append(vec)\n","        return len(new_vec)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"aXvOibJOqFjZ","executionInfo":{"status":"ok","timestamp":1630504875339,"user_tz":-330,"elapsed":557,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"367803e2-3cf2-46b7-dccf-5a81c6012e28"},"source":["en_text.vocab.itos[3]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<eos>'"]},"metadata":{},"execution_count":73}]},{"cell_type":"code","metadata":{"id":"WdLP_HpRBBz7"},"source":["INPUT_SIZE = len(hi_text.vocab)  # source vocab size\n","OUTPUT_SIZE = len(en_text.vocab)  # target vocab size\n","ENC_EMBED_SIZE = 512\n","DEC_EMBED_SIZE = 256\n","HIDDEN_SIZE = 512\n","N_LAYERS = 1\n","DROPOUT_RATE = 0.5\n","N_EPOCHS = 50\n","CLIP = 0.5\n","PAD_IDX = 1\n","UNK_IDX = 0\n","SOS_IDX = 2\n","EOS_IDX = 3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DS9gVm3TBB42"},"source":["def convert_ids_to_text(ids, vocab, eos_idx, unk_idx):\n","    \"\"\" Converts token ids to text.\n","    \"\"\"\n","    if ids.dim() == 1:\n","        output_tokens = []\n","        for token_id in ids:\n","            if token_id == eos_idx:\n","                break\n","            else:\n","                output_tokens.append(vocab.itos[token_id])\n","        return output_tokens\n","    \n","    elif ids.dim() == 2:\n","        return [convert_ids_to_text(ids[:, i], vocab, eos_idx, unk_idx) for i in range(ids.size(1))]\n","    \n","    raise RuntimeError(f'ids has {ids.size()} dimensions, expected 2 dimensions')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"krK347Cgclhg"},"source":["def train_fn(model, iterator, optimizer, criterion, clip=1.0):\n","    \n","    \n","    total_loss = 0\n","    steps = 0\n","    model.train()\n","    tk0 = tqdm(iterator, total=len(iterator), position=0, leave=True)\n","    \n","    for idx, batch in enumerate(tk0):\n","        source = batch.Hindi\n","        target = batch.roman\n","\n","        # source: (source_seq_len, batch_size), source_lengths: (batch_size)\n","        # target: (target_seq_len, batch_size), target_lengths: (batch_size)\n","\n","        # forward pass\n","        optimizer.zero_grad()\n","        output = model(source, target)  # (target_seq_len, batch_size, output_size)\n","        \n","        # ignoring SOS_TOKEN\n","        output = output[1:]  # ((target_seq_len - 1), batch_size, output_size)\n","        target = target[1:]  # ((target_seq_len - 1), batch_size)\n","        \n","        # calculate the loss\n","        loss = criterion(\n","            output.view(-1, output.shape[-1]),  # ((target_seq_len - 1) * batch_size, output_size)\n","            target.view(-1)  # ((target_seq_len - 1) * batch_size)\n","        )\n","        total_loss += loss.item()\n","        steps += 1\n","\n","        output = output.argmax(dim=-1)  # ((target_seq_len - 1), batch_size)\n","\n","        # backward pass\n","        loss.backward()\n","\n","        # clip gradients to avoid exploding gradients issue\n","        nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","        # update model parameters\n","        optimizer.step()\n","        \n","        tk0.set_postfix(loss=total_loss/steps)\n","        \n","    tk0.close()\n","\n","    perplexity = np.exp(total_loss / len(iterator))\n","    \n","    return output, perplexity\n","def eval_fn(model, iterator, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    steps = 0\n","    hypotheses = []\n","    references = []\n","    \n","    tk0 = tqdm(iterator, total=len(iterator), position=0, leave=True)\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(tk0):\n","            source = batch.Hindi\n","            target= batch.roman\n","            \n","            # source: (source_seq_len, batch_size), source_lengths: (batch_size)\n","            # target: (target_seq_len, batch_size), target_lengths: (batch_size)\n","            \n","            # forward pass\n","            output = model(source,target)  # (target_seq_len, batch_size, output_size)\n","            \n","            # ignoring SOS_TOKEN\n","            output = output[1:]  # ((target_seq_len - 1), batch_size, output_size)\n","            target = target[1:]  # ((target_seq_len - 1), batch_size)\n","            \n","            # calculate the loss\n","            loss = criterion(\n","                output.view(-1, output.shape[-1]),  # ((target_seq_len - 1) * batch_size, output_size)\n","                target.view(-1)  # ((target_seq_len - 1) * batch_size)\n","            )\n","            total_loss += loss.item()\n","            steps += 1\n","            \n","            output = output.argmax(dim=-1)  # ((target_seq_len - 1), batch_size)\n","\n","            # converting the ids to tokens (used later for calculating BLEU score)\n","            pred_tokens = convert_ids_to_text(output, en_text.vocab, EOS_IDX, UNK_IDX)\n","            target_tokens = convert_ids_to_text(target, en_text.vocab, EOS_IDX, UNK_IDX)\n","\n","            hypotheses += pred_tokens\n","            references += [[token] for token in target_tokens]\n","            \n","            tk0.set_postfix(loss=total_loss/steps)\n","            \n","    tk0.close()\n","            \n","    perplexity = np.exp(total_loss / len(iterator))\n","    bleu4 = bleu_score(hypotheses, references)\n","\n","    return output, perplexity, bleu4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r-2QDWF-tWUM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tdu3hMjBBBsO"},"source":["cls = Encoder(input_size=INPUT_SIZE, embed_size=ENC_EMBED_SIZE, hidden_size =HIDDEN_SIZE, n_layers=N_LAYERS, dropout_rate=DROPOUT_RATE )\n","dls = DecoderWithAttention(output_size =OUTPUT_SIZE , embed_size = DEC_EMBED_SIZE, hidden_size=HIDDEN_SIZE, n_layers=N_LAYERS, dropout_rate=DROPOUT_RATE )\n","model = Seq2SeqWithAttention(cls,dls,pad_idx=PAD_IDX)\n","if device == 'cuda':\n","  model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RohVrClJtWMX"},"source":["for i in train_dataloader:\n","  x = cls(i.Hindi,i.roman)\n","  break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SwseyAsXtWWi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0UDkyKqte2bk"},"source":["optimizer = torch.optim.RMSprop(model.parameters(), lr=3e-2)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer, patience=3, threshold=0.001, mode='max'\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O9L4Q8ZSexrq"},"source":["criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":599,"referenced_widgets":["78e96dd5a54e4653906c4ad52623dd8e","b1775ebcfc5740ffbf2b614ace704276","66d1d9fa9dc041e9ba99335b5894c517","16dcf11054a247448c4e43f70d7869b0","71a1c1eacb874027b3f1ef66118310fe","7b98eefd9b704bc7b53b9854a94442b9","cc97f5dbcd67451ba084d9784f55cabe","3abb5bd435b24978ac5200f0becfdb07","decc780c8f424951bf0d81633f39fabf","321c139f2cc0432895a1d11f167e952e","a19adf5f4be6460d9e012d5b2816b22a","1ca4eafdbb51418aac1ae6939ca274af","3fe0189606f44afe80d4a61998fbd17e","9b682875356a455a8db61276b834ad02","04f6ec7e1fe841dda2ddc51fce330a92","7015ad1ad3c2421685abbd9f38b1ae66","a87cf3eccb254b9a93b98441da55fecb","9036d52d841d4081a3ae2f0d83f9ace0","6793b2d937494b2d8ba3fd7b249abb07","2d27342055b64878a0eb488b0ad525d4","17451388bc1448fb9d5e043e945f1904","d03856e9bee6421da9ee3f2eb77bb8d5","b702e448426c44e099703f97b74c961b","4f63dbf3eff94967a317f9a3215fb065","2322172f049a4640a41e2a485e94f210","d47c6eb6d4054fa2b7e43a31d69f5d19","80a7cdbd1b6448e99a4b91b938e5dbd7","714bb83d8f064cc2b65961f12b0216ba","c99ed233581a40b98bd2f69385646663","1768925ed49f4aefa1d3fec7fdd8921b","879535abb8864238adfbccc402f75d39","9f2c768b27ba46bf880c217beb81ec43","3be64572218c452aa880fde3362f75ae","2a9c16b22e304f0c8ea7709c5f9b9114","75764f14953f45728ddd93c5af8345df"]},"id":"OioP4LUvjsHX","executionInfo":{"status":"error","timestamp":1630511573029,"user_tz":-330,"elapsed":137737,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"7e0f2169-dbfe-4f37-9f6f-dfe13bcbe5e4"},"source":["best_bleu4 = float('-inf')\n","es_patience = 10\n","\n","patience = 0\n","model_path = 'model(rms).pth'\n","\n","\n","for epoch in range(0, N_EPOCHS + 1):\n","    # one epoch training\n","    _, train_perplexity = train_fn(model, train_dataloader, optimizer, criterion, CLIP)\n","    \n","    # one epoch validation\n","    _, valid_perplexity, valid_bleu4 = eval_fn(model, val_dataloader, criterion)\n","    \n","    print(f'Epoch: {epoch}, Train perplexity: {train_perplexity:.4f}, Valid perplexity: {valid_perplexity:.4f}, Valid BLEU4: {valid_bleu4:.4f}')\n","    \n","    scheduler.step(valid_bleu4)\n","    \n","    # early stopping\n","    is_best = valid_bleu4 > best_bleu4\n","    if is_best:\n","        print(f'BLEU score improved ({best_bleu4:.4f} -> {valid_bleu4:.4f}). Saving Model!')\n","        best_bleu4 = valid_bleu4\n","        patience = 0\n","        torch.save(model.state_dict(), model_path)\n","    else:\n","        patience += 1\n","        print(f'Early stopping counter: {patience} out of {es_patience}')\n","        if patience == es_patience:\n","            print(f'Early stopping! Best BLEU4: {best_bleu4:.4f}')\n","            break\n"],"execution_count":null,"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78e96dd5a54e4653906c4ad52623dd8e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/138 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ca4eafdbb51418aac1ae6939ca274af","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch: 0, Train perplexity: 10606674071876.3184, Valid perplexity: 86572276389.1072, Valid BLEU4: 0.0000\n","BLEU score improved (-inf -> 0.0000). Saving Model!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3fe0189606f44afe80d4a61998fbd17e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/138 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b682875356a455a8db61276b834ad02","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/16 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 1, Train perplexity: 11465944001486.9590, Valid perplexity: 81349205712.7947, Valid BLEU4: 0.0000\n","Early stopping counter: 1 out of 10\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2322172f049a4640a41e2a485e94f210","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/138 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-ee83c4aa0e0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_EPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# one epoch training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_perplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# one epoch validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-f3d9959a91af>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# clip gradients to avoid exploding gradients issue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UulMKGwytWYT","executionInfo":{"status":"ok","timestamp":1630511647131,"user_tz":-330,"elapsed":579,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"303e4f19-6773-48b2-ca99-eddbf99d2aed"},"source":["model.eval()\n","for i in val_dataloader:\n","  input = i.Hindi\n","  target = i.roman\n","  output = model(input,target)\n","  output = output[1:]  # ((target_seq_len - 1), batch_size, output_size)\n","  target = target[1:]\n","  print(output.shape)\n","  output = output.argmax(dim=-1) \n","  pred_words = convert_ids_to_text(output, en_text.vocab, EOS_IDX, UNK_IDX)\n","  actual = convert_ids_to_text(target, en_text.vocab, EOS_IDX, UNK_IDX)\n","  print(pred_words)\n","  \n","  break\n","  "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([49, 32, 21955])\n","[['aisi', 'ek', 'Sachivalay', 'ko', 'paryay', 'madhyam', 'paryay', 'madhyam', 'paryay', 'madhyam', 'paryay', 'madhyam', 'prakar', 'prakar', 'ek', 'Sachivalay', 'utarti', 'aarambh', 'Sachivalay', 'prakar', 'ek', 'Sachivalay', 'prakar', 'ek', 'prakar', 'prakar', 'ek', 'Sachivalay', 'ho', 'paryay', 'Sachivalay', 'prakar', 'utarti', 'prakar', 'prakar', 'ek', 'Sachivalay', 'prakar', 'prakar', 'prakar', 'ek', 'Sachivalay', 'prakar', 'ek', 'Sachivalay', 'ho', 'prakar', 'ek', 'Sachivalay'], ['Sachivalay', 'sanvad', 'Peshewar', 'Sachivalay', 'bheji', 'usa', 'ki.', '(v', 'naam', 'Sachivalay', 'prakash', 'liye', 'Sachivalay', 'prakash', 'liye', 'Sachivalay', 'Mishra', 'paryay', 'Sachivalay', 'tara', 'tara', 'paryay', 'Sachivalay', 'paryay', 'Sachivalay', 'Sachivalay', 'Peshewar', 'Sachivalay', 'paryay', 'paryay', 'paryay', 'paryay', 'paryay', 'Peshewar', 'bheji', 'paryay', 'paryay', 'bodo,', 'Peshewar', 'bheji', 'paryay', 'paryay', 'Peshewar', 'Sachivalay', 'paryay', 'paryay', 'Peshewar', 'Sachivalay', 'paryay'], [], ['keval', 'hota', 'Faruvaahi', 'hota', 'se', 'Samanya', 'se', 'hota', 'se', 'Samanya', 'se', 'Samanya', 'se', 'se', 've', 'hota', 'se', 'Ameriki', 'hota', 'se', 'hai', 'hota', 'se', 'hai', 'se', 'se', 've', 'Patiyala', 'hota', 'se', 'Wil', 'se', 'Wil', 'se', 'se', 've', 'Patiyala', 'hota', 'se', 'se', 've', 'Patiyala', 'se', 've', 'Patiyala', 'hota', 'se', 've', '1994'], [], ['Wil', 'khasa', 'mahavir', 'acanaka', 'upayog', 'degree', 'mool', 'puratatwa', 'puratatwa', 'bharat', 'Wil', 'December,', 'upayog', 'mool', 'upayog', 'degree', 'upayog', 'degree', 'December,', 'upayog', 'Sachivalay', 'upayog', 'Wil', 'puratatwa', 'upayog', 'puratatwa', 'puratatwa', 'puratatwa', 'puratatwa', 'puratatwa', 'puratatwa', 'puratatwa', 'puratatwa', 'Wil', 'upayog', 'degree', 'December,', 'upayog', 'upayog', 'upayog', 'Sachivalay', 'upayog', 'Wil', 'puratatwa', 'puratatwa', 'puratatwa', 'Wil', 'December,', 'upayog'], ['janma', 'kaha', 'hai', 'upanyas', 'addhyayan', 'addhyayan', 'hota'], ['Sachivalay', 'mukhya', 'mukhya', 'Sachivalay', 'mukhya', 'mukhya', 'pita', 'mukhya', 'mukhya', 'ka', 'mukhya', 'mukhya', 'ka', 'mukhya', 'mukhya', 'ka', 'tallin'], [], ['hai', 'koniy', 'khasa', 'khasa', 'hai', 'hai', 'koshikayen', 'khasa', 'hai', 'hai', 'saken.', 'koniy'], ['mask', 'mask', '7', 'yah', 'yah', 'yah', 'Faruvaahi', 'hoti', 'kiya'], ['bhakton', 'bhasha', 'jahan', 'sare'], ['pure', 'jenar', 'pure', 'usa', '(v', 'paryay', '7', 'aisi', 'tha', 'aisi', 'Hindi', 'aisi', 'tha', '7', '7', '7', '7', '7', '1994', 'aisi', 'paryay', 'darbar,', 'mool', 'aisi', 'mool', 'mool', 'tallin', 'aisi', 'tha', 'aisi', 'tha', 'aisi', 'tha', 'mool', 'mool', 'aisi', 'tha', 'aisi', 'mool', 'mool', 'aisi', 'tha', 'mool', 'aisi', 'tha', 'aisi', 'mool', 'aisi', 'paryay'], ['daayod', 'bhasha', 'antariksh', 'Peshewar', 'kiya', 'huye', 'lagi', 'kiya', 'lagi', 'kiya', 'kiya', 'lagi', 'kiya', 'kiya', 'lagi', 'kiya', 'lagi', 'kiya', 'lagi', 'kiya', 'Ameriki', 'kiya', 'nepali,', 'kiya', 'kiya', 'kiya', 'Ameriki', 'kiya', 'Ameriki', 'kiya', 'Ameriki', 'kiya', 'Ameriki', 'kiya', 'kiya', 'huye', 'kiya', 'Ameriki', 'kiya', 'kiya', 'huye', 'kiya', 'kiya', 'huye', 'kiya', 'nepali,', 'kiya', 'huye', 'kiya'], ['dhup,', 'visheshagy', 'huye', 'darbar,', 'visheshagy', 'December,', 'Vigyani', 'godbole', 'Vigyani', 'huye', 'Vigyani', 'udghatan', 'Mishra', 'December,', 'huye', 'hota', 'me', 'hota', 'ki', 'Mishra', 'duniya', 'sare', 'December,', 'visheshagy', 'visheshagy', 'huye', 'visheshagy', 'duniya', 'sare', 'udghatan', 'ka', 'sare', 'udghatan', 'duniya', 'lie', 'duniya', 'sare', 'udghatan', 'udghatan', 'aag', 'udghatan', 'huye', 'duniya', 'sare', 'udghatan', 'huye', 'ka', 'saken.', 'sse'], ['bheji', 'men', 'sthit', 'me', 'sthit', 'Ameriki', 'ki', 'sthit', 'sthit', 'Arts', 'sthit', 'sthit', 'sthit', 'sthit', 'sthit', 'sthit', 'sthit', 'sthit', 'sthit', 'punjabi,', 'sthit', 'sthit', 'sthit', 'sthit', 'ki', 'sthit', 'sthit', 'sthit', 'sthit', 'sthit', 'sthit', 'sthit', 'sthit', 'ki', 'sthit', 'sthit', 'sthit', 'sthit', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki', 'ki'], ['sihre', 'parivartit'], ['marg', 'bhakton', 'bhakton', 'chandra', 'bhakton', 'chandra', 'hain.', 'bhakton', 'tha', 'chandra', 'chandra', 'raja'], ['ke', 'bhasha', 'batata', 'hota', 'vedhshala', 'bhasha', 'vedhshala', 'men', 'ke', 'ek', 'bhashayen', 'ek', 'aag', 'bhashayen', 'ek', 'aag', 'bhashayen', 'ek', 'aag', 'bhashayen', 'iksinga', 'vedhshala', 'batata', 'vedhshala', 'vedhshala', 'vedhshala', 'ke', 'ek', 'aag', 'bhashayen', 'ek', 'aag', 'bhashayen', 'vedhshala', 'vedhshala', 'bhashayen', 'karte', 'khuduwa,', 'vedhshala', 'vedhshala', 'ke', 'ek', 'vedhshala', 'ke', 'ek', 'aag', 'vedhshala', 'bhashayen', 'iksinga'], ['khuduwa,', 'Sachivalay', 'bhakton', 'Sachivalay', 'maaf', 'hain,', 'hain,', 'hain,', 'hain,', 'Mishra', 'hain,', 'Mishra', 'hain,', 'bangla,', 'hain,', 'Mishra', 'hain,', 'Mishra', '7', 'Sachivalay', 'hain,', 'Mishra', '7', 'hain,', 'Faruvaahi', 'hain,', 'Mishra', 'hain,', 'Mishra', 'hain,', 'Mishra', 'hain,', 'Mishra', 'hain,', 'usa', 'hain,', 'Mishra', 'hain,', 'se', 'se', 'hain,', 'Mishra', 'se', 'hain,', 'Mishra', 'hain,', 'se', 'hain,', 'Mishra'], ['iksinga', 'B', 'bhartiya', 'tamil,', 'tallin', 'vedhshala', 'tamil,', 'upanyas', 'upanyas', 'upanyas', 'B', 'B', 'upanyas', 'upanyas', 'vedhshala', 'niyamit', 'B', 'Hindi', 'B', 'vedhshala', 'niyamit', 'B', 'vedhshala', 'tallin', 'vedhshala', 'niyamit', 'B', 'Hindi', 'B', 'upanyas', 'vedhshala', 'niyamit', 'B', 'vedhshala', 'vedhshala', 'tallin', 'vedhshala', 'niyamit', 'vedhshala', 'vedhshala', 'tallin', 'vedhshala', 'vedhshala', 'tallin', 'vedhshala', 'niyamit', 'vedhshala', 'maaf', 'prasidh'], ['tallin', 've', 'evm', 've', 'hota', 'tallin', 'niyamit', 'niyamit', 'tallin', 'iksinga', 'tallin', 'iksinga', 'tallin', 'iksinga', 'tallin', 'rachi', 'iksinga', 'tallin', 'kiya.', 'tallin', 'kiya.', 'hota', 'iksinga', 'tallin', 'iksinga', 'tallin', 'kiya.', 'hota', 'tallin', 'iksinga', 'tallin', 'kiya.', 'hota', 'iksinga', 'iksinga', 'tallin', 'kiya.', 'hota', 'tallin', 'iksinga', 'tallin', 'kiya.', 'tallin', 'kiya.', 'hota', 'se', 'iksinga', 'tallin', 'kiya.'], ['hain.', 'tallin', 'ghar,', 'madhyam', 'kiya', 'kahte', 'khadera', 'mukhya', 'prakash', 'madhyam', 'mukhya', 'kshetra', 'mukhya', 'mukhya', 'kshetra', 'mukhya', 'kshetra', 'mukhya', 'kshetra', 'madhyam', 'mukhya', 'kshetra', 'madhyam', 'mukhya', 'madhyam', 'madhyam', 'mukhya', 'kshetra', 'mukhya', 'kshetra', 'mukhya', 'kshetra', 'mukhya', 'madhyam', 'madhyam', 'mukhya', 'kshetra', 'mukhya', 'madhyam', 'madhyam', 'mukhya', 'kshetra', 'madhyam', 'mukhya', 'kshetra', 'mukhya', 'madhyam', 'mukhya', 'kshetra'], ['Wil', 'sarvapratham', 'Wil'], [], ['hai.', 'daayod', 'chapat', 'paryay', 'hoti', 'aag', 'hoti', 'hoti', 'aag', 'hoti', 'Mishra', 'uske', 'devi-devta', 'sse', 'devi-devta', 'devi-devta', 'devi-devta', 'devi-devta', 'devi-devta', 'sihre', 'hoti', 'hai.', 'mein', 'bhakton', 'mein', 'mein', 'upanyas', 'sse', 've', 'upanyas', 'sihre', 'hoti', 'aag', 'hai.', 'mein', 'hai.', 'sthit', '(v', 'hai.', 'mein', 'khadera', 'Arts', 'hai.', 'sthit', '(v', 'devi-devta', 'hai.', 'sthit', 'devi-devta'], ['hain.', 'Mishra', 'Mishra', 'pr', 'Mishra', 'Mishra', 'tanava', 'tallin', 'tanava', 'pr', 'Mishra', 'karta', 'tanava', 'tallin', 'tallin', 'Vinod', 'tanava', 'pr', 'Mishra', 'Mishra', 'karta', 'Mishra', 'tanava', 'tanava', '(v', 'tanava', 'tanava', 'tanava', 'tanava', 'tanava', 'tanava', 'tanava', 'tanava', 'tallin', 'tanava', 'tanava', 'tanava', 'tanava', 'tanava', 'tanava', 'kahte', 'tanava', 'tanava', 'kahte', '(v', 'tallin', 'tanava', 'kahte', 'tanava'], ['bheji', 'jodkar'], ['sthaan', 'pita', 'dhobiyauva,', 'tallin', 'tallin', '(v', 'asli', 'tallin', 'tallin', 'tallin', 'tallin', 'tallin', 'tallin', 'tallin', 'tallin', '(v', '(v', 'tallin', '(v', 'tallin', '(v', '(v', 'sthaan', 'sthaan', 'sthaan', 'acanaka', 'hai', 'tallin', 'tallin', 'tallin', '(v', '(v', 'tallin', 'sthaan', '\"vishalkaay\"', 'tallin', '(v', 'par', 'sthaan', 'mahavir', 'tallin', 'tallin', 'sthaan', 'pita', 'lakshmi', 'sthaan', 'sthaan', 'tallin', 'tallin'], ['Wil', 'se', 'Wil', 'ghar,', 'ghar,', 'taki'], ['tha', 'tha', 'madhyam', 'ne'], ['kiya', 'pure', 'ki', 'hai', 'kiya', 'kiya', 'tallin', 'sse', 'prakar', 'mem', 'eka', 'uske', 'madhyam', 'mem', 'bangla,', 'ki', 'yah', 'ki', 'yah', 'prakar', 'mem', 'sse', 'pita', 'mahavir', 'gai.', 'uske', 'tallin', 'kiya', 'prakat', 'hain.', 'sthit', 'sare', 'tallin', 'se', 'se', 'tallin', 'tallin', 'ling', 'hai.', 'tallin', 'iksinga', 'para', 'uske', 'prakat', 'uske', 'saken.', 'tallin', 'tallin', 'tallin']]\n"]}]},{"cell_type":"code","metadata":{"id":"5GS6R8XjQApW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YzipdqHdQArt"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2bak-6Sz8sZF","colab":{"base_uri":"https://localhost:8080/","height":236},"executionInfo":{"status":"error","timestamp":1630488842527,"user_tz":-330,"elapsed":8,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"066187ed-d667-4f30-bfdc-12db621a1bb5"},"source":["native_uni = []\n","native_bi = []\n","for line in native_corpus:\n","  #words = words.split(chr(2404))\n","  #words[0] += \" \"+chr(2404)\n","  #words = words[0].split(\" \")\n","  words = line.split(\" \")\n","  native_uni.append(words)\n","  #bi_words = [\"\".join(words[i:i+2]) for i in range(0,len(words)-1,1)]\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-b846c963adc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnative_uni\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnative_bi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnative_corpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;31m#words = words.split(chr(2404))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#words[0] += \" \"+chr(2404)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'native_corpus' is not defined"]}]},{"cell_type":"code","metadata":{"id":"QlS2K1ON9LFd"},"source":["del native_corpus\n","del df2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GSUQUnmfENX_"},"source":["native_uni"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GlxnteM83jQA","executionInfo":{"status":"ok","timestamp":1629995196997,"user_tz":-330,"elapsed":1085,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"5f0d6215-4785-44bd-f3cd-0241e6a7486a"},"source":["import gensim\n","from gensim.models import FastText\n","from gensim.models.word2vec import Word2Vec\n","# training Word2Vec\n","# Skip gram word to vec \n","# uni grams , bigrams, \n","\n","\n","cores = multiprocessing.cpu_count()\n","cores\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"ldJv9F3i3kVa"},"source":["\n","fast_t = FastText(native_uni, size=150, window=5, min_count=4, workers=cores,sg=1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"chjsTjb1BV96"},"source":["w2v = gensim.models.word2vec.Word2Vec(native_uni, size=150,   \n","            window= 8, min_count=5, sg=1, iter=15,workers=cores)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oSUOYZxhDVfN"},"source":["w2v.save(pth_w2v_r_2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vbKjEBBfDvJx","executionInfo":{"status":"ok","timestamp":1630001592198,"user_tz":-330,"elapsed":478,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"9d70a0d9-4e53-4aa9-e890-7f6736517f51"},"source":["print(w2v.similar_by_word(\"age\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('Halanki,', 0.9745258092880249), ('aadhaar', 0.9696657061576843), ('prabhu', 0.9691759347915649), ('laga.', 0.9690280556678772), ('dhan', 0.9684903025627136), ('halanki,', 0.9662507772445679), ('sambhavana', 0.9659699201583862), ('sagar', 0.9651817679405212), ('pushti', 0.9646499156951904), ('paise', 0.9644907116889954)]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-1qEkuvZDx_e","executionInfo":{"status":"ok","timestamp":1629999473998,"user_tz":-330,"elapsed":2336,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"1167d015-69a5-414e-d333-6ad1be412ac5"},"source":["!pip install upgrade gensim \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement upgrade (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for upgrade\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"maoawOmiKZfV"},"source":["\n","w2v = Word2Vec.load(pth_w2v_1)\n","#w2v_r = Word2Vec.load(pth_w2v_r_2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ax__w1qDjxkC"},"source":["from tensorflow.keras import models, layers, preprocessing as kprocessing\n","from tensorflow.keras import backend as K"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mkaXEuhJggnF"},"source":["tokenizer_2 = kprocessing.text.Tokenizer(lower=True, split=' ', \n","                     oov_token=\"<UNK>\", \n","                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n","tokenizer_2.fit_on_texts(native_corpus)\n","dic_vocabulary = tokenizer_2.word_index\n","## create sequence\n","lst_text2seq= tokenizer_2.texts_to_sequences(native_corpus)\n","## padding sequence\n","#X_train = kprocessing.sequence.pad_sequences(lst_text2seq, \n"," #                   maxlen=50, padding=\"post\", truncating=\"post\")\n","Y_train = kprocessing.sequence.pad_sequences(lst_text2seq, \n","                   maxlen=50, padding=\"post\", truncating=\"post\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WvysskYhKqG7","executionInfo":{"status":"ok","timestamp":1630001818687,"user_tz":-330,"elapsed":487,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"53f59000-aaec-4144-9242-57858613be5c"},"source":["Y_train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[   51,   602,     6, ...,     0,     0,     0],\n","       [ 6279,     4,  3827, ...,     0,     0,     0],\n","       [ 1260,  6281,    23, ...,     0,     0,     0],\n","       ...,\n","       [   67, 18547,     6, ...,     0,     0,     0],\n","       [  259, 18551,  1411, ...,     0,     0,     0],\n","       [18554,    40,     2, ...,     0,     0,     0]], dtype=int32)"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"ZIpxHIgvjbQg"},"source":["import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xVENVvLyxa9z","executionInfo":{"status":"ok","timestamp":1630001916072,"user_tz":-330,"elapsed":406,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"7620eced-e9d1-41dd-f6b8-050113c450da"},"source":["embedd_native = np.zeros((len(dic_vocabulary)+1, 150))\n","for word,idx in dic_vocabulary.items():\n","    ## update the row with vector\n","    try:\n","        embedd_native[idx] =  w2v[word]\n","    ## if word not in model then skip and the row stays all 0s\n","    except:\n","        pass"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  \"\"\"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"M5uWAYhmlO6u"},"source":["#Dump save embeddings roman and native\n","with open(path_embedw2v_150_roman, 'wb') as handle:\n","  pickle.dump(embedd_native, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ySCvH7FqnbZ8"},"source":["#load embeddings\n","with open(path_embedw2v_150_native, 'rb') as handle:\n","  embeddings = pickle.load(handle)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L4jKry5nmcti"},"source":["# jsonify tokenizer and save with io buffer\n","jso = tokenizer_2.to_json()\n","with io.open(path_tokenizer_roman,'w', encoding='utf-8')  as f:\n","  f.write(json.dumps(jso, ensure_ascii=False))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U5lmL0qTmJrF"},"source":["# load tokenizer from keras \n","from keras_preprocessing.text import tokenizer_from_json\n","with open(tokenizer_path) as f:\n","  data = json.load(f)\n","  tokenizer = tokenizer_from_json(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yERIvZv6nU0T"},"source":["#save X_train Y train\n","with open(path_ytrain_roman, 'wb') as handle:\n","  pickle.dump(Y_train, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IJ_n2EBzoDTw"},"source":["w2v.predict_output_word()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WZNTee_8pWB0"},"source":[""],"execution_count":null,"outputs":[]}]}