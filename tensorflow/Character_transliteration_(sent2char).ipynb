{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Character_transliteration_(sent2char).ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1sBhgLhh4g5DbUNd5vhmmugp4sFx7kd7i","authorship_tag":"ABX9TyP0wTzojrSnRyhR6TBDl7qf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RRwbJqVO-B_g","executionInfo":{"status":"ok","timestamp":1631446435300,"user_tz":-330,"elapsed":5325,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"aa54cddd-6579-47fe-871e-86a8ec484647"},"source":["!pip install tensorflow_text"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow_text\n","  Downloading tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 7.2 MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n","Requirement already satisfied: tensorflow<2.7,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.6.0)\n","Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (2.6.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.12.1)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.15.0)\n","Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (2.6.0)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.7.4.3)\n","Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (5.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.1.0)\n","Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.39.0)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.12)\n","Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.19.5)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.37.0)\n","Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.1.0)\n","Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (2.6.0)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.2.0)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.4.0)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (0.12.0)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.3.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.17.3)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.6.3)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (1.1.2)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.5.2)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (57.4.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.4.5)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.0.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2.23.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.6.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.34.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (4.2.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (4.6.4)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.5.0)\n","Installing collected packages: tensorflow-text\n","Successfully installed tensorflow-text-2.6.0\n"]}]},{"cell_type":"code","metadata":{"id":"UFg-N5QVjt5N","executionInfo":{"status":"ok","timestamp":1631446437563,"user_tz":-330,"elapsed":2269,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["\n","import pandas as pd\n","import numpy as np\n","import os, io, json\n","import pickle\n","\n","import typing\n","from typing import Any, Tuple\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers.experimental import preprocessing\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","import tensorflow_text as tf_text\n","from tensorflow.keras import models, layers, preprocessing as kprocessing\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"OIzvbEhSATEQ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1WPshWXm6IS8","executionInfo":{"status":"ok","timestamp":1631429182385,"user_tz":-330,"elapsed":8911,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"a131d108-2416-4a54-d5a4-165bbe103b1e"},"source":["\n","try: # detect TPUs\n","  tpu = None\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n","  tf.config.experimental_connect_to_cluster(tpu)\n","  tf.tpu.experimental.initialize_tpu_system(tpu)\n","  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","except ValueError: # detect GPUs\n","  #strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n","  #strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n","  strategy = tf.distribute.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n","\n","print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Clearing out eager caches\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Clearing out eager caches\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.124.33.26:8470\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.124.33.26:8470\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Finished initializing TPU system.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Finished initializing TPU system.\n","WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Found TPU system:\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Found TPU system:\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["Number of accelerators:  8\n"]}]},{"cell_type":"code","metadata":{"id":"Kqyka-10ATKS","executionInfo":{"status":"ok","timestamp":1631459827151,"user_tz":-330,"elapsed":414,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["# enter valid paths for hindi_joined.csv , model weights\n","\n","path_dataset = \"/content/drive/MyDrive/Dataset/Dakshina/hi/hindi_joined.csv\"\n","pth_modell =  \"/content/drive/MyDrive/Dataset/Dakshina/hi/tensorflow/sent_mod_1/weights.h5\""],"execution_count":123,"outputs":[]},{"cell_type":"code","metadata":{"id":"PABcfY1Def2C","executionInfo":{"status":"ok","timestamp":1631459881956,"user_tz":-330,"elapsed":436,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["class DataProcessExclusive():\n","  def __init__(self,filepath):\n","    self.df = pd.read_csv(filepath,index_col=[0])\n","    self.hindi = df['Hindi']\n","    self.roman = df['roman']\n","    self.tokenizer_hi = None\n","    self.tokenizer_en=None\n","    self.x = None\n","    self.y = None\n","    self.preprocess(4)\n","  \n","  def preprocess(self,max_words):\n","    self.hi = self.shorten(self.hindi,max_words)\n","    self.en = self.shorten(self.roman,max_words)\n","    \n","    hi2 = self.pre_hi(self.hi)\n","    en2 = self.pre_en(self.en)\n","    \n","    tokenizer_hi =  Tokenizer(num_words=5000,char_level=True,oov_token='<UNK>' )\n","    tokenizer_en =  Tokenizer(num_words=5000,char_level=True,oov_token='<UNK>' )\n","\n","    tokenizer_hi.fit_on_texts(hi2)\n","    tokenizer_en.fit_on_texts(en2)\n","\n","    self.tokenizer_hi = self.update_tokenizer(tokenizer_hi)\n","    self.tokenizer_en = self.update_tokenizer(tokenizer_en)\n","\n","    hi2 = self.tokenizer_hi.texts_to_sequences(hi2)\n","    en2 = self.tokenizer_en.texts_to_sequences(en2)\n","\n","    hi2 = self.add_spcl_tokens(hi2,self.tokenizer_hi.word_index)\n","    en2 = self.add_spcl_tokens(en2,self.tokenizer_en.word_index)\n","\n","    self.x = kprocessing.sequence.pad_sequences(hi2, \n","                     padding=\"post\", truncating=\"post\")\n","    self.y = kprocessing.sequence.pad_sequences(en2, \n","                     padding=\"post\", truncating=\"post\")\n","    return \n","\n","  def add_spcl_tokens(self,text,vocab):\n","    start_token = len(vocab)-1\n","    end_token = len(vocab)\n","    new_text = []\n","    for i in text:\n","        i = [start_token] + i + [end_token]\n","        new_text.append(i)\n","    return new_text\n","\n","  def update_tokenizer(self,tokenizer):\n","    start_token = len(tokenizer.index_word)+1\n","    end_token = len(tokenizer.index_word)+2\n","\n","    tokenizer.index_word[start_token] = '<sos>'\n","    tokenizer.word_index['<sos>'] = start_token\n","    tokenizer.index_word[end_token] = '<eos>'\n","    tokenizer.word_index['<eos>'] = end_token\n","    \n","    return tokenizer\n","  def pre_en(self,texts):\n","    new_text = []\n","    for text in texts:\n","      text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n","    \n","      text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n","    \n","      text = tf.strings.strip(text)\n","      text = text.numpy().decode()\n","      new_text.append(text)\n","    return new_text\n","\n","  def pre_hi(self,texts):\n","    new_text = []\n","    for text in texts:\n","      text = tf.strings.strip(text)\n","      text = text.numpy().decode()\n","      new_text.append(text)\n","    return new_text\n","\n","  def shorten(self,text,max_words):\n","    new = []\n","    for i in text :\n","      _wor = i.split(\" \")      \n","      l=max_words\n","      for k in range(0,len(_wor)):     \n","        if (k%max_words == 0) :\n","          new.append(\"  \".join(_wor[k:k+l]))         \n","          l=0    \n","        l+=1   \n","    return new"],"execution_count":125,"outputs":[]},{"cell_type":"code","metadata":{"id":"HIu63IxptKgl","executionInfo":{"status":"ok","timestamp":1631460418516,"user_tz":-330,"elapsed":1096,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n","    super(Encoder, self).__init__()\n","    self.enc_units = enc_units\n","    self.input_vocab_size = input_vocab_size\n","    \n","    \n","    # The embedding layer converts tokens to vectors\n","    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n","                                               embedding_dim)\n","\n","    # The GRU RNN layer processes those vectors sequentially.\n","    \n","    self.gru1 = tf.keras.layers.Bidirectional( tf.keras.layers.GRU(units=self.enc_units,reset_after=True, dropout=0.5,return_sequences=True,return_state=True))\n","    self.gru = tf.keras.layers.GRU(self.enc_units,\n","                                   dropout=0.4,\n","                                   # Return the sequence and state\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform', reset_after=True)\n","    self.dense = tf.keras.layers.Dense(units=self.enc_units,use_bias=False)\n","  def call(self, tokens, state=None):\n","    #tokens = self.input1(tokens)\n","    #shape_checker = ShapeChecker()\n","    #shape_checker(tokens, ('batch', 's'))\n","\n","    # 2. The embedding layer looks up the embedding for each token.\n","    vectors = self.embedding(tokens)\n","    #shape_checker(vectors, ('batch', 's', 'embed_dim'))\n","\n","    # 3. The GRU processes the embedding sequence.\n","    #    output shape: (batch, s, enc_units)\n","    #    state shape: (batch, enc_units)\n","    \n","    output, s1,s2 = self.gru1(vectors, initial_state=state)\n","    \n","    output, state = self.gru(output, initial_state=state)\n","    state = tf.keras.layers.concatenate([s1,s2,state], axis=1)\n","    state = self.dense(state)\n","    \n","    #shape_checker(output, ('batch', 's', 'enc_units'))\n","    #shape_checker(state, ('batch', 'enc_units'))\n","\n","    # 4. Returns the new sequence and its state.\n","    return output, state\n","\n","class BahdanauAttention(tf.keras.layers.Layer):\n","  def __init__(self, units):\n","    super().__init__()\n","    # For Eqn. (4), the  Bahdanau attention\n","    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n","    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n","\n","    self.attention = tf.keras.layers.AdditiveAttention()\n","\n","  def call(self, query, value, mask):\n","    \n","    # From Eqn. (4), `W1@ht`.\n","    w1_query = self.W1(query)\n","    # ('batch', 't', 'attn_units'))\n","\n","    # From Eqn. (4), `W2@hs`.\n","    w2_key = self.W2(value)\n","    #('batch', 's', 'attn_units'))\n","\n","    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n","    value_mask = mask\n","\n","    context_vector, attention_weights = self.attention(\n","        inputs = [w1_query, value, w2_key],\n","        mask=[query_mask, value_mask],\n","        return_attention_scores = True,\n","    )\n","    #context_vector, ('batch', 't', 'value_units'))\n","    #attention_weights, ('batch', 't', 's'))\n","\n","    return context_vector, attention_weights\n","\n","# run this \n","\n","class DecoderInput(typing.NamedTuple):\n","  new_tokens: Any\n","  enc_output: Any\n","  mask: Any\n","\n","class DecoderOutput(typing.NamedTuple):\n","  logits: Any\n","  attention_weights: Any\n","\n","  \n","class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n","    super(Decoder, self).__init__()\n","    self.dec_units = dec_units\n","    self.output_vocab_size = output_vocab_size\n","    self.embedding_dim = embedding_dim\n","\n","    # For Step 1. The embedding layer convets token IDs to vectors\n","    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n","                                               embedding_dim)\n","\n","    # For Step 2. The RNN keeps track of what's been generated so far.\n","    self.gru = tf.keras.layers.GRU(self.dec_units, dropout=0.25,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","\n","    self.gru2 = tf.keras.layers.GRU(self.dec_units, dropout=0.6,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    # For step 3. The RNN output will be the query for the attention layer.\n","    self.attention = BahdanauAttention(self.dec_units)\n","\n","    # For step 4. Eqn. (3): converting `ct` to `at`\n","    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n","                                    use_bias=False)\n","\n","    # For step 5. This fully connected layer produces the logits for each\n","    # output token.\n","    self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n","\n","\n","  def call(self,\n","         inputs: DecoderInput,\n","         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n","  \n","    #if state is not None:\n","      \n","  # Step 1. Lookup the embeddings\n","    vectors = self.embedding(inputs.new_tokens)\n","    #shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n","\n","  # Step 2. Process one step with the RNN\n","    rnn_output, s1 = self.gru(vectors, initial_state=state)\n","    rnn_output, state = self.gru2(rnn_output, initial_state=state)\n","    #shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n","    #shape_checker(state, ('batch', 'dec_units'))\n","\n","  # Step 3. Use the RNN output as the query for the attention over the\n","  # encoder output.\n","    context_vector, attention_weights = self.attention(\n","        query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n","  #shape_checker(context_vector, ('batch', 't', 'dec_units'))\n","  #shape_checker(attention_weights, ('batch', 't', 's'))\n","\n","  # Step 4. Eqn. (3): Join the context_vector and rnn_output\n","  #     [ct; ht] shape: (batch t, value_units + query_units)\n","    context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n","\n","  # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n","    attention_vector = self.Wc(context_and_rnn_output)\n"," # shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n","\n","  # Step 5. Generate logit predictions:\n","    logits = self.fc(attention_vector)\n","  #shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n","\n","    return DecoderOutput(logits, attention_weights), state\n","\n","    \n","class TrainTranslator(tf.keras.Model):\n","  def __init__(self, embedding_dim, units,\n","               input_text_vocab,\n","               output_text_vocab, \n","               use_tf_function=True):\n","    super().__init__()\n","    # Build the encoder and decoder\n","    self.input_text_vocab = input_text_vocab\n","    self.output_text_vocab = output_text_vocab\n","\n","    encoder = Encoder((len(self.input_text_vocab)+1),\n","                      embedding_dim, units)\n","    decoder = Decoder((len(output_text_vocab)+1),\n","                      embedding_dim, units)\n","\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    \n","    self.use_tf_function = use_tf_function\n","    #self.shape_checker = ShapeChecker()\n","\n","  def train_step(self, inputs):\n","    #self.shape_checker = ShapeChecker()\n","    if self.use_tf_function:\n","      return self._tf_train_step(inputs)\n","    else:\n","      return self._train_step(inputs)\n","  def _preprocess(self, input_tokens, target_tokens):\n","    #self.shape_checker(input_text, ('batch',))\n","    #self.shape_checker(target_text, ('batch',))\n","\n","    # Convert the text to token IDs\n","    #input_tokens = self.input_text_processor(input_text)\n","    #target_tokens = self.output_text_processor(target_text)\n","    #self.shape_checker(input_tokens, ('batch', 's'))\n","    #self.shape_checker(target_tokens, ('batch', 't'))\n","\n","    # Convert IDs to masks.\n","    input_mask = input_tokens != 0\n","    #self.shape_checker(input_mask, ('batch', 's'))\n","\n","    target_mask = target_tokens != 0\n","    #self.shape_checker(target_mask, ('batch', 't'))\n","\n","    return input_tokens, input_mask, target_tokens, target_mask\n","  \n","  def call(self,inputs):\n","    input_mask = inputs != 0\n","    enc_output, enc_state = self.encoder(inputs)\n","    dec_state = enc_state\n","    max_len = 60\n","    output = tf.zeros(shape=[inputs.shape[0], max_len, (len(self.output_text_vocab)+1)],dtype=tf.float32)\n","    input_token = tf.ones(shape=[inputs.shape[0],1], dtype = tf.int32 ) * self.output_text_vocab['<sos>']\n","    output = tf.unstack(output,axis=1)\n","    for i in range(max_len - 1):\n","      \n","      decoder_input = DecoderInput(new_tokens=input_token,\n","                               enc_output=enc_output,\n","                               mask=input_mask)\n","      dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n","      input_token = tf.argmax(dec_result.logits,axis=-1) \n","      \n","      \n","      output[i] = tf.squeeze(dec_result.logits,axis=1)\n","    \n","    output = tf.stack(output,axis=1)\n","      \n","    return output\n","\n","  def _train_step(self, inputs):\n","    input_text, target_text = inputs  \n","\n","    (input_tokens, input_mask,target_tokens, target_mask) = self._preprocess(input_text, target_text)\n","\n","    max_target_length = target_tokens.shape[1]\n","\n","    with tf.GradientTape() as tape:\n","    # Encode the input\n","      enc_output, enc_state = self.encoder(input_tokens)\n","      #self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n","      #self.shape_checker(enc_state, ('batch', 'enc_units'))\n","\n","      #  Initialize the decoder's state to the encoder's final state.\n","    # This only works if the encoder and decoder have the same number of\n","    # units.\n","      dec_state = enc_state\n","      loss = tf.constant(0.0)\n","\n","      for t in range(max_target_length-1):\n","        # Pass in two tokens from the target sequence:\n","      # 1. The current input to the decoder.\n","      # 2. The target the target for the decoder's next prediction.\n","        \n","        new_tokens = target_tokens[:, t:t+2]\n","        step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n","                                             enc_output, dec_state)\n","        loss = loss + step_loss\n","\n","    # Average the loss over all non padding tokens.\n","        average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n","\n","  # Apply an optimization step\n","    variables = self.trainable_variables \n","    gradients = tape.gradient(average_loss, variables)\n","    self.optimizer.apply_gradients(zip(gradients, variables))\n","\n","  # Return a dict mapping metric names to current value\n","    return {'batch_loss': average_loss}\n","\n","\n","  def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n","    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n","    \n","    # Run the decoder one step.\n","    decoder_input = DecoderInput(new_tokens=input_token,\n","                               enc_output=enc_output,\n","                               mask=input_mask)\n","\n","    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n","    \n","\n","    #self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n","    #self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n","    #self.shape_checker(dec_state, ('batch', 'dec_units'))\n","\n","  # `self.loss` returns the total for non-padded tokens\n","    y = target_token\n","    y_pred = dec_result.logits\n","    step_loss = self.loss(y, y_pred)\n","\n","    return step_loss, dec_state\n","  \n","  @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.int32, shape=[None,]),\n","                               tf.TensorSpec(dtype=tf.int32, shape=[None,])]])\n","  def _tf_train_step(self, inputs):\n","    return self._train_step(inputs)\n","\n","class MaskedLoss(tf.keras.losses.Loss):\n","  def __init__(self):\n","    self.name = 'masked_loss'\n","    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True, reduction='none')\n","\n","  def __call__(self, y_true, y_pred):\n","    #shape_checker = ShapeChecker()\n","    #shape_checker(y_true, ('batch', 't'))\n","    #shape_checker(y_pred, ('batch', 't', 'logits'))\n","\n","    # Calculate the loss for each item in the batch.\n","    loss = self.loss(y_true, y_pred)\n","    #shape_checker(loss, ('batch', 't'))\n","\n","    # Mask off the losses on padding.\n","    mask = tf.cast(y_true != 0, tf.float32)\n","    #shape_checker(mask, ('batch', 't'))\n","    loss *= mask\n","\n","    # Return the total.\n","    return tf.reduce_sum(loss)"],"execution_count":131,"outputs":[]},{"cell_type":"code","metadata":{"id":"vjgdCbhFmGzt","executionInfo":{"status":"ok","timestamp":1631459997478,"user_tz":-330,"elapsed":413,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["class Translator_new(tf.Module):\n","\n","  def __init__(self, model,input_tokenizer,output_tokenizer):\n","    self.model = model\n","    self.input_tokenizer = input_tokenizer\n","    self.output_tokenizer = output_tokenizer\n","    #self.output_token_string_from_index = output_tokenizer.sequences_to_texts()\n","    #self.output_token_string_from_index = (\n","     #   tf.keras.layers.experimental.preprocessing.Char(\n","      #      vocabulary=output_text_vocab,\n","       #     mask_token='',\n","       #     invert=True))\n","\n","    # The output should never generate padding, unknown, or start.\n","    #index_from_string = tf.keras.layers.experimental.preprocessing.StringLookup(\n","       # vocabulary=output_tokenizer.word_index, mask_token='')\n","    token_mask_ids = 0\n","\n","    token_mask = np.zeros([len(output_tokenizer.word_index)+1], dtype=np.bool)\n","    token_mask[np.array(token_mask_ids)] = True\n","    self.token_mask = token_mask\n","\n","    self.start_token_en = output_tokenizer.word_index['<sos>']\n","    self.end_token_en = output_tokenizer.word_index['<eos>']\n","\n","    self.start_token_hi = input_tokenizer.word_index['<sos>']\n","    self.end_token_hi = input_tokenizer.word_index['<eos>']\n","\n","  def translate(self,input_text,*,max_chars=100):\n","    input_text = self.preprocess_texts(input_text,lang='hi')\n","    logits = self.model(input_text)\n","    preds = tf.argmax(logits,axis=-1)\n","    \n","    \n","    texts = self.preprocess_seq(preds.numpy().tolist())\n","    #print(texts)\n","    return texts\n","  def preprocess_seq(self,seq):\n","    vocab = self.output_tokenizer.index_word\n","    for i,j in enumerate(seq):\n","      for o,k in enumerate(j):  \n","        \n","        if (k == self.end_token_en):\n","          new_j = \"\".join([vocab[index] for index in j[:o]])\n","          \n","          seq[i] = new_j\n","          break\n","         \n","    return seq\n","\n","  def preprocess_texts(self,input_text,lang='hi'):\n","    if lang == 'hi':\n","      \n","      input_text = self.input_tokenizer.texts_to_sequences(input_text)\n","      for i,j in enumerate(input_text):\n","         j = [self.start_token_hi] + j + [self.end_token_hi]\n","         input_text[i] = j\n","      input_text = kprocessing.sequence.pad_sequences(input_text,padding=\"post\", truncating=\"post\")\n","      return input_text\n","  \n","  \n","  def load_translator_weights(self,filepath):\n","    #2d input \n","    input = tf.ones(shape=[10,20],dtype=tf.int32)\n","    #call model once\n","    self.model(input)\n","    self.model.load_weights(filepath)\n","    return self.model\n","\n","  def load_translator(self,filepath):\n","    embedding_dim =64\n","    units =1024\n","    pass"],"execution_count":128,"outputs":[]},{"cell_type":"code","metadata":{"id":"MUVDaW_plPTO","executionInfo":{"status":"ok","timestamp":1631459898140,"user_tz":-330,"elapsed":7872,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":[""],"execution_count":126,"outputs":[]},{"cell_type":"code","metadata":{"id":"yL1UFhuIloBM"},"source":["#load dataset -> (dataframe) -> preprocess -> tokenize\n","\n","#data.x :                   x_train \n","#data.y :                   y_train\n","#data.hi :                  shortened_to_max_word_sentences : List[List]\n","#data.en :                  shortened_to_max_word_sentences : List[List]\n","#data.tokenizer_hi :        input_tokenizer from keras.kpreprocess\n","#data.tokenizer_en :        output_tokenizer from keras.kpreprocess\n","#data.tokenizer_hi.word_index: vocab\n","\n","data = DataProcessExclusive(path_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_QKRJhDDoIB0","executionInfo":{"status":"ok","timestamp":1631436707044,"user_tz":-330,"elapsed":1608077,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"2e9a2e83-e729-4a6d-9f9d-c4b01f2cabfc"},"source":["# train the model\n","embedding_dim, units = 64, 1024\n","\n","\n","with strategy.scope():\n","  translator = TrainTranslator(\n","    embedding_dim, units,\n","    input_text_vocab = data.tokenizer_hi.word_index,\n","    output_text_vocab = data.tokenizer_en.word_index,\n","    use_tf_function=False)\n","\n","# Configure the loss and optimizer\n","  translator.compile(\n","    optimizer=tf.optimizers.Adam(),\n","    loss=MaskedLoss(),\n","  )\n","training = translator.fit(x= data.x, y= data.y, batch_size=128, \n","                     epochs=25,  shuffle=True, verbose=1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","175/175 [==============================] - 718s 1s/step - batch_loss: 3.0559\n","Epoch 2/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 1.9168\n","Epoch 3/25\n","175/175 [==============================] - 37s 211ms/step - batch_loss: 1.5095\n","Epoch 4/25\n","175/175 [==============================] - 37s 211ms/step - batch_loss: 0.5215\n","Epoch 5/25\n","175/175 [==============================] - 37s 211ms/step - batch_loss: 0.3153\n","Epoch 6/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.2688\n","Epoch 7/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.2470\n","Epoch 8/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.2313\n","Epoch 9/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.2186\n","Epoch 10/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1965\n","Epoch 11/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1943\n","Epoch 12/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1717\n","Epoch 13/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1613\n","Epoch 14/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1479\n","Epoch 15/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1404\n","Epoch 16/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1345\n","Epoch 17/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1195\n","Epoch 18/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1077\n","Epoch 19/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1023\n","Epoch 20/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.0887\n","Epoch 21/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.0840\n","Epoch 22/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.0721\n","Epoch 23/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.0621\n","Epoch 24/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.0698\n","Epoch 25/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.0551\n"]}]},{"cell_type":"code","metadata":{"id":"Zv2Phg88n4CE"},"source":["#save the new model weights\n","pth_model_2 = \"/content/drive/MyDrive/Dataset/Dakshina/hi/tensorflow/sent_mod_2\"\n","translator_1 = training.model\n","save_model(translator_1,pth_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VBXgF1VuytuM","executionInfo":{"status":"ok","timestamp":1631460164490,"user_tz":-330,"elapsed":4219,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["# instanciate the Translator_new class which transliterates a given texts in hindi to roman \n","if not translator:\n","  translator = None\n","translator_obj = Translator_new(\n","    model=translator,\n","    input_tokenizer=data.tokenizer_hi,\n","    output_tokenizer=data.tokenizer_en,\n",")\n","#load saved model weights to translator_obj.model\n","\n","translator = translator_obj.load_translator_weights(filepath = pth_modell)"],"execution_count":129,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eqI82M7UmKUv"},"source":[""]},{"cell_type":"code","metadata":{"id":"sb8hwCjKdn0Q"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OTMuLzzQytue","executionInfo":{"status":"ok","timestamp":1631460170782,"user_tz":-330,"elapsed":3191,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"6d918527-4431-4c35-c6f3-cf9d6c5ba1cb"},"source":["#test predictions\n","\n","print(translator_obj.translate(data.hi[5:8]))\n","print( data.en[5:8])"],"execution_count":130,"outputs":[{"output_type":"stream","name":"stdout","text":["['gunkaari  tikhur', 'cichael  kamen  the  wall', 'ydymayamaya  hisso  ke']\n","['gunkaari  tikhur', 'Michael  kamen  (the  wall', 'ke  vadyamaya  hisso  ke']\n"]}]},{"cell_type":"code","metadata":{"id":"_ZKEWGMIn4JE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fHWtKcXWn4N7"},"source":["### From here it is simply roughs . No need to go thru \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DnaQ99T6ATMp","executionInfo":{"status":"ok","timestamp":1631446439664,"user_tz":-330,"elapsed":1571,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["df = pd.read_csv(path_dataset,index_col=[0])"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"lO7RldZIYzAv","executionInfo":{"status":"ok","timestamp":1631446650352,"user_tz":-330,"elapsed":577,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["en = df['roman']\n","hi = df['Hindi']\n","def shorten(hi,en,max_words):\n","  new_hi = []\n","  new_en = []\n","  for i,j in zip(hi,en):\n","    hi_wor = i.split(\" \")\n","    en_wor = j.split(\" \")\n","    l=4\n","    for k in range(0,len(hi_wor)):\n","      \n","      if (k%max_words == 0) :\n","        new_hi.append(\"  \".join(hi_wor[k:k+l]))\n","        new_en.append(\"  \".join(en_wor[k:k+l]))\n","        l=0\n","      #elif k == len(hi_wor)-1:\n","      #  new_hi.append(hi_wor[k-l:])\n","      l+=1\n","    \n","  return new_hi,new_en\n","hi,en = shorten(hi,en,4)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jRwRWD_hFyNJ","executionInfo":{"status":"ok","timestamp":1631446608078,"user_tz":-330,"elapsed":3425,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"aabfac1c-2c1d-4046-8b0e-ca8d69e76e17"},"source":["#preprocess \n","\n","def pre_hi(texts):\n","  new_text = []\n","  for text in texts:\n","    #text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n","    # Add spaces around punctuation.\n","    #text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n","    # Strip whitespace.\n","    text = tf.strings.strip(text)\n","    text = text.numpy().decode()\n","    new_text.append(text)\n","  return new_text\n","\n","hi = pre_hi(hi)\n","tokenizer_hi = Tokenizer(num_words=5000,\n","    char_level=True,oov_token='<UNK>' \n",")\n","tokenizer_hi.fit_on_texts(hi)\n","hi = tokenizer_hi.texts_to_sequences(hi)\n","\n","start_token_hi = len(tokenizer_hi.index_word)+1\n","end_token_hi = len(tokenizer_hi.index_word)+2\n","\n","print(start_token_hi,end_token_hi)\n","new_text_hi = []\n","for i in hi:\n","  i = [start_token_hi] + i + [end_token_hi]\n","  new_text_hi.append(i)\n","\n","tokenizer_hi.index_word[start_token_hi] = '<sos>'\n","tokenizer_hi.word_index['<sos>'] = start_token_hi\n","tokenizer_hi.index_word[end_token_hi] = '<eos>'\n","tokenizer_hi.word_index['<eos>'] = end_token_hi\n","\n","\n","x =kprocessing.sequence.pad_sequences(new_text_hi, \n","                     padding=\"post\", truncating=\"post\")"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["156 157\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WMzRHRMyK2n8","executionInfo":{"status":"ok","timestamp":1631446614066,"user_tz":-330,"elapsed":4326,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"ea63d1df-e51a-4aa7-c4f1-ddd38b2c417b"},"source":["#preprocess roman text\n","\n","def pre_en(texts):\n","  new_text = []\n","  for text in texts:\n","    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n","    # Add spaces around punctuation.\n","    text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n","    # Strip whitespace.\n","    text = tf.strings.strip(text)\n","    text = text.numpy().decode()\n","    new_text.append(text)\n","  return new_text\n","\n","\n","en = pre_en(en)\n","tokenizer_en = Tokenizer( filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n',\n","    char_level=True,oov_token='<UNK>',\n",")\n","tokenizer_en.fit_on_texts(en)\n","en = tokenizer_en.texts_to_sequences(en)\n","\n","start_token = len(tokenizer_en.index_word)+1\n","end_token = len(tokenizer_en.index_word)+2\n","print(start_token,end_token)\n","new_text_en = []\n","for i in en:\n","  i = [start_token] + i + [end_token]\n","  new_text_en.append(i)\n","\n","tokenizer_en.index_word[start_token] = '<sos>'\n","tokenizer_en.word_index['<sos>'] = start_token\n","tokenizer_en.index_word[end_token] = '<eos>'\n","tokenizer_en.word_index['<eos>'] = end_token\n","\n","y = kprocessing.sequence.pad_sequences(new_text_en, \n","                     padding=\"post\", truncating=\"post\")"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["33 34\n"]}]},{"cell_type":"code","metadata":{"id":"jUohQEbf7zXu","executionInfo":{"status":"ok","timestamp":1631429382134,"user_tz":-330,"elapsed":366,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["# Use a word level tokenizer instead but with the vocab of previous char level tokenizer.\n","tokenizer_hi2 = Tokenizer(num_words=5000,\n","    char_level=False,oov_token='<UNK>' \n",")\n","tokenizer_en2 = Tokenizer(num_words=5000,\n","    char_level=False,oov_token='<UNK>' \n",")"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"taQeeZc7e7Pa","executionInfo":{"status":"ok","timestamp":1631429383621,"user_tz":-330,"elapsed":3,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["tokenizer_hi2.word_index =tokenizer_hi.word_index\n","tokenizer_hi2.index_word =tokenizer_hi.index_word\n","\n","tokenizer_en2.word_index =tokenizer_en.word_index\n","tokenizer_en2.index_word =tokenizer_en.index_word"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"cI6Q3mZFfJNO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jqeCnUGylpWy","executionInfo":{"status":"ok","timestamp":1631429441519,"user_tz":-330,"elapsed":370,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["def _save_weights(model,save_folder):\n","    save_path = os.path.join(save_folder , \"weights.h5\")\n","    model.save_weights(save_path)\n","\n","def _create_folder_if_it_dosent_exist(folder):\n","    if not os.path.exists(folder):\n","      os.makedirs(folder)\n","def save_model(model, save_folder):\n","    _create_folder_if_it_dosent_exist(save_folder)\n","    #_save_parameters(model, save_folder)\n","    _save_weights(model,save_folder)\n"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D5NurW3UGI3_","executionInfo":{"status":"ok","timestamp":1631449454555,"user_tz":-330,"elapsed":713,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"bffca8aa-c952-44a1-c96a-404a5a03367e"},"source":["translator_obj.end_token_en"],"execution_count":81,"outputs":[{"output_type":"execute_result","data":{"text/plain":["34"]},"metadata":{},"execution_count":81}]},{"cell_type":"code","metadata":{"id":"3RaPIdLJBTGk"},"source":["y[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"46pVqsZhBqRT","executionInfo":{"status":"ok","timestamp":1631250876472,"user_tz":-330,"elapsed":401,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":[""],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"j2cIt0rFngO7"},"source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n","    super(Encoder, self).__init__()\n","    self.enc_units = enc_units\n","    self.input_vocab_size = input_vocab_size\n","    \n","    \n","    # The embedding layer converts tokens to vectors\n","    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n","                                               embedding_dim)\n","\n","    # The GRU RNN layer processes those vectors sequentially.\n","    \n","    self.gru1 = tf.keras.layers.Bidirectional( tf.keras.layers.GRU(units=self.enc_units,reset_after=True, dropout=0.5,return_sequences=True,return_state=True))\n","    self.gru = tf.keras.layers.GRU(self.enc_units,\n","                                   dropout=0.4,\n","                                   # Return the sequence and state\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform', reset_after=True)\n","    self.dense = tf.keras.layers.Dense(units=self.enc_units,use_bias=False)\n","  def call(self, tokens, state=None):\n","    #tokens = self.input1(tokens)\n","    #shape_checker = ShapeChecker()\n","    #shape_checker(tokens, ('batch', 's'))\n","\n","    # 2. The embedding layer looks up the embedding for each token.\n","    vectors = self.embedding(tokens)\n","    #shape_checker(vectors, ('batch', 's', 'embed_dim'))\n","\n","    # 3. The GRU processes the embedding sequence.\n","    #    output shape: (batch, s, enc_units)\n","    #    state shape: (batch, enc_units)\n","    \n","    output, s1,s2 = self.gru1(vectors, initial_state=state)\n","    \n","    output, state = self.gru(output, initial_state=state)\n","    state = tf.keras.layers.concatenate([s1,s2,state], axis=1)\n","    state = self.dense(state)\n","    \n","    #shape_checker(output, ('batch', 's', 'enc_units'))\n","    #shape_checker(state, ('batch', 'enc_units'))\n","\n","    # 4. Returns the new sequence and its state.\n","    return output, state"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SIN2lH25wdqo","executionInfo":{"status":"ok","timestamp":1631424820130,"user_tz":-330,"elapsed":407,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["embedding_dim =256\n","units =1024"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bp5XJhpOINYH","executionInfo":{"status":"ok","timestamp":1631424821506,"user_tz":-330,"elapsed":13,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"0824c733-0e36-4486-8a87-564a7fc8f144"},"source":["len(tokenizer_hi.word_index)"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["157"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"bjRlxXnztNdn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631424849303,"user_tz":-330,"elapsed":5733,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"aabe243d-36f0-4e4d-ba2d-0c1317f71da9"},"source":["# Convert the input text to tokens.\n","example_tokens = x[:64]\n","\n","# Encode the input sequence.\n","encoder = Encoder(len(tokenizer_hi.word_index)+1,\n","                  embedding_dim, units)\n","example_enc_output, example_enc_state = encoder(example_tokens)\n","\n","#print(f'Input batch, shape (batch): {example_input_batch.shape}')\n","print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n","print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n","print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')\n"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Input batch tokens, shape (batch, s): (64, 60)\n","Encoder output, shape (batch, s, units): (64, 60, 1024)\n","Encoder state, shape (batch, units): (64, 1024)\n"]}]},{"cell_type":"code","metadata":{"id":"9cgPn4PI5uAu","executionInfo":{"status":"ok","timestamp":1631447950520,"user_tz":-330,"elapsed":594,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["class BahdanauAttention(tf.keras.layers.Layer):\n","  def __init__(self, units):\n","    super().__init__()\n","    # For Eqn. (4), the  Bahdanau attention\n","    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n","    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n","\n","    self.attention = tf.keras.layers.AdditiveAttention()\n","\n","  def call(self, query, value, mask):\n","    \n","    # From Eqn. (4), `W1@ht`.\n","    w1_query = self.W1(query)\n","    # ('batch', 't', 'attn_units'))\n","\n","    # From Eqn. (4), `W2@hs`.\n","    w2_key = self.W2(value)\n","    #('batch', 's', 'attn_units'))\n","\n","    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n","    value_mask = mask\n","\n","    context_vector, attention_weights = self.attention(\n","        inputs = [w1_query, value, w2_key],\n","        mask=[query_mask, value_mask],\n","        return_attention_scores = True,\n","    )\n","    #context_vector, ('batch', 't', 'value_units'))\n","    #attention_weights, ('batch', 't', 's'))\n","\n","    return context_vector, attention_weights\n","# run this \n","\n","class DecoderInput(typing.NamedTuple):\n","  new_tokens: Any\n","  enc_output: Any\n","  mask: Any\n","\n","class DecoderOutput(typing.NamedTuple):\n","  logits: Any\n","  attention_weights: Any\n","\n","  \n","class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n","    super(Decoder, self).__init__()\n","    self.dec_units = dec_units\n","    self.output_vocab_size = output_vocab_size\n","    self.embedding_dim = embedding_dim\n","\n","    # For Step 1. The embedding layer convets token IDs to vectors\n","    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n","                                               embedding_dim)\n","\n","    # For Step 2. The RNN keeps track of what's been generated so far.\n","    self.gru = tf.keras.layers.GRU(self.dec_units, dropout=0.25,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","\n","    self.gru2 = tf.keras.layers.GRU(self.dec_units, dropout=0.6,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    # For step 3. The RNN output will be the query for the attention layer.\n","    self.attention = BahdanauAttention(self.dec_units)\n","\n","    # For step 4. Eqn. (3): converting `ct` to `at`\n","    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n","                                    use_bias=False)\n","\n","    # For step 5. This fully connected layer produces the logits for each\n","    # output token.\n","    self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n","\n","\n","  def call(self,\n","         inputs: DecoderInput,\n","         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n","  \n","    #if state is not None:\n","      \n","  # Step 1. Lookup the embeddings\n","    vectors = self.embedding(inputs.new_tokens)\n","    #shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n","\n","  # Step 2. Process one step with the RNN\n","    rnn_output, s1 = self.gru(vectors, initial_state=state)\n","    rnn_output, state = self.gru2(rnn_output, initial_state=state)\n","    #shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n","    #shape_checker(state, ('batch', 'dec_units'))\n","\n","  # Step 3. Use the RNN output as the query for the attention over the\n","  # encoder output.\n","    context_vector, attention_weights = self.attention(\n","        query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n","  #shape_checker(context_vector, ('batch', 't', 'dec_units'))\n","  #shape_checker(attention_weights, ('batch', 't', 's'))\n","\n","  # Step 4. Eqn. (3): Join the context_vector and rnn_output\n","  #     [ct; ht] shape: (batch t, value_units + query_units)\n","    context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n","\n","  # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n","    attention_vector = self.Wc(context_and_rnn_output)\n"," # shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n","\n","  # Step 5. Generate logit predictions:\n","    logits = self.fc(attention_vector)\n","  #shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n","\n","    return DecoderOutput(logits, attention_weights), state"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"tav5qYOnIYpt"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wXXeVsDxrfgp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631425041939,"user_tz":-330,"elapsed":444,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"8e08531e-11f3-420f-994d-8a55def996dc"},"source":["attention_layer = BahdanauAttention(units)\n","# Later, the decoder will generate this attention query\n","example_attention_query = tf.random.normal(shape=[len(example_tokens), 1, 10])\n","\n","# Attend to the encoded tokens\n","\n","context_vector, attention_weights = attention_layer(\n","    query=example_attention_query,\n","    value=example_enc_output,\n","    mask=(example_tokens != 0))\n","\n","print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n","print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Attention result shape: (batch_size, query_seq_length, units):           (64, 1, 1024)\n","Attention weights shape: (batch_size, query_seq_length, value_seq_length): (64, 1, 60)\n"]}]},{"cell_type":"code","metadata":{"id":"u18AvAHdrVrG","colab":{"base_uri":"https://localhost:8080/","height":299},"executionInfo":{"status":"ok","timestamp":1631424972449,"user_tz":-330,"elapsed":458,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"336517ec-e71e-43c6-f19f-e90ec5e1b7b7"},"source":["plt.subplot(1, 2, 1)\n","plt.pcolormesh(attention_weights[:, 1, :])\n","plt.title('Attention weights')\n","\n","plt.subplot(1, 2, 2)\n","plt.pcolormesh(example_tokens != 0)\n","plt.title('Mask')"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'Mask')"]},"metadata":{},"execution_count":27},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"Ezy5sP0wg2Wj","executionInfo":{"status":"ok","timestamp":1631429400286,"user_tz":-330,"elapsed":371,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["#do not run this\n","class DecoderInput(typing.NamedTuple):\n","  new_tokens: Any\n","  enc_output: Any\n","  mask: Any\n","\n","class DecoderOutput(typing.NamedTuple):\n","  logits: Any\n","  attention_weights: Any\n","\n","  \n","class Decoder2(tf.keras.layers.Layer):\n","  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n","    super(Decoder2, self).__init__()\n","    self.dec_units = dec_units\n","    self.output_vocab_size = output_vocab_size\n","    self.embedding_dim = embedding_dim\n","\n","    # For Step 1. The embedding layer convets token IDs to vectors\n","    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n","                                               embedding_dim)\n","\n","    # For Step 2. The RNN keeps track of what's been generated so far.\n","    self.gru = tf.keras.layers.GRU(self.dec_units, dropout=0.25,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","\n","    self.gru2 = tf.keras.layers.GRU(self.dec_units, dropout=0.6,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    # For step 3. The RNN output will be the query for the attention layer.\n","    self.attention = BahdanauAttention(self.dec_units)\n","\n","    # For step 4. Eqn. (3): converting `ct` to `at`\n","    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n","                                    use_bias=False)\n","\n","    # For step 5. This fully connected layer produces the logits for each\n","    # output token.\n","    self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n","\n","\n","  def call(self,\n","         inputs: DecoderInput,\n","         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n","  \n","    #if state is not None:\n","      \n","  # Step 1. Lookup the embeddings\n","    vectors = self.embedding(inputs.new_tokens)\n","    #shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n","\n","  # Step 2. Process one step with the RNN\n","    rnn_output, s1 = self.gru(vectors, initial_state=state)\n","    \n","    #shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n","    #shape_checker(state, ('batch', 'dec_units'))\n","\n","  # Step 3. Use the RNN output as the query for the attention over the\n","  # encoder output.\n","    context_vector, attention_weights = self.attention(\n","        query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n","  #shape_checker(context_vector, ('batch', 't', 'dec_units'))\n","  #shape_checker(attention_weights, ('batch', 't', 's'))\n","  # step 3.5 : Feed the context vectors to rnn 2\n","    rnn_output, state = self.gru2(context_vector, initial_state=state)\n","  # Step 4. Eqn. (3): Join the context_vector and rnn_output\n","  #     [ct; ht] shape: (batch t, value_units + query_units)\n","    context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n","\n","  # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n","    attention_vector = self.Wc(context_and_rnn_output)\n"," # shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n","\n","  # Step 5. Generate logit predictions:\n","    logits = self.fc(attention_vector)\n","  #shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n","\n","    return DecoderOutput(logits, attention_weights), state"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZCqJPQxZ8i73","executionInfo":{"status":"ok","timestamp":1631447824818,"user_tz":-330,"elapsed":455,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["# run this \n","\n","class DecoderInput(typing.NamedTuple):\n","  new_tokens: Any\n","  enc_output: Any\n","  mask: Any\n","\n","class DecoderOutput(typing.NamedTuple):\n","  logits: Any\n","  attention_weights: Any\n","\n","  \n","class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n","    super(Decoder, self).__init__()\n","    self.dec_units = dec_units\n","    self.output_vocab_size = output_vocab_size\n","    self.embedding_dim = embedding_dim\n","\n","    # For Step 1. The embedding layer convets token IDs to vectors\n","    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n","                                               embedding_dim)\n","\n","    # For Step 2. The RNN keeps track of what's been generated so far.\n","    self.gru = tf.keras.layers.GRU(self.dec_units, dropout=0.25,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","\n","    self.gru2 = tf.keras.layers.GRU(self.dec_units, dropout=0.6,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    # For step 3. The RNN output will be the query for the attention layer.\n","    self.attention = BahdanauAttention(self.dec_units)\n","\n","    # For step 4. Eqn. (3): converting `ct` to `at`\n","    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n","                                    use_bias=False)\n","\n","    # For step 5. This fully connected layer produces the logits for each\n","    # output token.\n","    self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n","\n","\n","  def call(self,\n","         inputs: DecoderInput,\n","         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n","  \n","    #if state is not None:\n","      \n","  # Step 1. Lookup the embeddings\n","    vectors = self.embedding(inputs.new_tokens)\n","    #shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n","\n","  # Step 2. Process one step with the RNN\n","    rnn_output, s1 = self.gru(vectors, initial_state=state)\n","    rnn_output, state = self.gru2(rnn_output, initial_state=state)\n","    #shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n","    #shape_checker(state, ('batch', 'dec_units'))\n","\n","  # Step 3. Use the RNN output as the query for the attention over the\n","  # encoder output.\n","    context_vector, attention_weights = self.attention(\n","        query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n","  #shape_checker(context_vector, ('batch', 't', 'dec_units'))\n","  #shape_checker(attention_weights, ('batch', 't', 's'))\n","\n","  # Step 4. Eqn. (3): Join the context_vector and rnn_output\n","  #     [ct; ht] shape: (batch t, value_units + query_units)\n","    context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n","\n","  # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n","    attention_vector = self.Wc(context_and_rnn_output)\n"," # shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n","\n","  # Step 5. Generate logit predictions:\n","    logits = self.fc(attention_vector)\n","  #shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n","\n","    return DecoderOutput(logits, attention_weights), state"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zv992WAr-Ps5","executionInfo":{"status":"ok","timestamp":1631425571840,"user_tz":-330,"elapsed":625,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["decoder = Decoder2((len(tokenizer_en.index_word)+1),\n","                  embedding_dim, units)"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"2cuDddsh-Pv5","executionInfo":{"status":"ok","timestamp":1631425550079,"user_tz":-330,"elapsed":426,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["# Convert the target sequence, and collect the \"[START]\" tokens\n","example_output_tokens = y[:64]\n","\n","#start_index = token.index('[START]')\n","first_token = tf.constant([[len(tokenizer_en.index_word)]] * example_output_tokens.shape[0])"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SUL9OxM3-P06","executionInfo":{"status":"ok","timestamp":1631425574773,"user_tz":-330,"elapsed":444,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"52b1e3ab-5b61-4b9c-9a86-c76747b00fd2"},"source":["# Run the decoder\n","dec_result, dec_state = decoder(\n","    inputs = DecoderInput(new_tokens=first_token,\n","                          enc_output=example_enc_output,\n","                          mask=(example_tokens != 0)),\n","    state = example_enc_state\n",")\n","\n","print(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\n","print(f'state shape: (batch_size, dec_units) {dec_state.shape}')"],"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["logits shape: (batch_size, t, output_vocab_size) (64, 1, 35)\n","state shape: (batch_size, dec_units) (64, 1024)\n"]}]},{"cell_type":"code","metadata":{"id":"GKDAmqrD-P3O"},"source":["sampled_token = tf.random.categorical(dec_result.logits[:,0,:],num_samples=1)\n","dec_result, dec_state = decoder(\n","    inputs = DecoderInput(new_tokens=sampled_token,\n","                          enc_output=example_enc_output,\n","                          mask=(example_tokens != 0)),\n","    state = example_enc_state\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3aRJymrZaFDl","executionInfo":{"status":"ok","timestamp":1631447833685,"user_tz":-330,"elapsed":1279,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["class MaskedLoss(tf.keras.losses.Loss):\n","  def __init__(self):\n","    self.name = 'masked_loss'\n","    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True, reduction='none')\n","\n","  def __call__(self, y_true, y_pred):\n","    #shape_checker = ShapeChecker()\n","    #shape_checker(y_true, ('batch', 't'))\n","    #shape_checker(y_pred, ('batch', 't', 'logits'))\n","\n","    # Calculate the loss for each item in the batch.\n","    loss = self.loss(y_true, y_pred)\n","    #shape_checker(loss, ('batch', 't'))\n","\n","    # Mask off the losses on padding.\n","    mask = tf.cast(y_true != 0, tf.float32)\n","    #shape_checker(mask, ('batch', 't'))\n","    loss *= mask\n","\n","    # Return the total.\n","    return tf.reduce_sum(loss)"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z5THwdkkSQij","executionInfo":{"status":"ok","timestamp":1631447836734,"user_tz":-330,"elapsed":448,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["class TrainTranslator(tf.keras.Model):\n","  def __init__(self, embedding_dim, units,\n","               input_text_vocab,\n","               output_text_vocab, \n","               use_tf_function=True):\n","    super().__init__()\n","    # Build the encoder and decoder\n","    self.input_text_vocab = input_text_vocab\n","    self.output_text_vocab = output_text_vocab\n","\n","    encoder = Encoder((len(self.input_text_vocab)+1),\n","                      embedding_dim, units)\n","    decoder = Decoder((len(output_text_vocab)+1),\n","                      embedding_dim, units)\n","\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    \n","    self.use_tf_function = use_tf_function\n","    #self.shape_checker = ShapeChecker()\n","\n","  def train_step(self, inputs):\n","    #self.shape_checker = ShapeChecker()\n","    if self.use_tf_function:\n","      return self._tf_train_step(inputs)\n","    else:\n","      return self._train_step(inputs)\n","  def _preprocess(self, input_tokens, target_tokens):\n","    #self.shape_checker(input_text, ('batch',))\n","    #self.shape_checker(target_text, ('batch',))\n","\n","    # Convert the text to token IDs\n","    #input_tokens = self.input_text_processor(input_text)\n","    #target_tokens = self.output_text_processor(target_text)\n","    #self.shape_checker(input_tokens, ('batch', 's'))\n","    #self.shape_checker(target_tokens, ('batch', 't'))\n","\n","    # Convert IDs to masks.\n","    input_mask = input_tokens != 0\n","    #self.shape_checker(input_mask, ('batch', 's'))\n","\n","    target_mask = target_tokens != 0\n","    #self.shape_checker(target_mask, ('batch', 't'))\n","\n","    return input_tokens, input_mask, target_tokens, target_mask\n","  \n","  def call(self,inputs):\n","    input_mask = inputs != 0\n","    enc_output, enc_state = self.encoder(inputs)\n","    dec_state = enc_state\n","    max_len = 60\n","    output = tf.zeros(shape=[inputs.shape[0], max_len, (len(self.output_text_vocab)+1)],dtype=tf.float32)\n","    input_token = tf.ones(shape=[inputs.shape[0],1], dtype = tf.int32 ) * self.output_text_vocab['<sos>']\n","    output = tf.unstack(output,axis=1)\n","    for i in range(max_len - 1):\n","      \n","      decoder_input = DecoderInput(new_tokens=input_token,\n","                               enc_output=enc_output,\n","                               mask=input_mask)\n","      dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n","      input_token = tf.argmax(dec_result.logits,axis=-1) \n","      \n","      \n","      output[i] = tf.squeeze(dec_result.logits,axis=1)\n","    \n","    output = tf.stack(output,axis=1)\n","      \n","    return output\n","\n","  def _train_step(self, inputs):\n","    input_text, target_text = inputs  \n","\n","    (input_tokens, input_mask,target_tokens, target_mask) = self._preprocess(input_text, target_text)\n","\n","    max_target_length = target_tokens.shape[1]\n","\n","    with tf.GradientTape() as tape:\n","    # Encode the input\n","      enc_output, enc_state = self.encoder(input_tokens)\n","      #self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n","      #self.shape_checker(enc_state, ('batch', 'enc_units'))\n","\n","      #  Initialize the decoder's state to the encoder's final state.\n","    # This only works if the encoder and decoder have the same number of\n","    # units.\n","      dec_state = enc_state\n","      loss = tf.constant(0.0)\n","\n","      for t in range(max_target_length-1):\n","        # Pass in two tokens from the target sequence:\n","      # 1. The current input to the decoder.\n","      # 2. The target the target for the decoder's next prediction.\n","        \n","        new_tokens = target_tokens[:, t:t+2]\n","        step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n","                                             enc_output, dec_state)\n","        loss = loss + step_loss\n","\n","    # Average the loss over all non padding tokens.\n","        average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n","\n","  # Apply an optimization step\n","    variables = self.trainable_variables \n","    gradients = tape.gradient(average_loss, variables)\n","    self.optimizer.apply_gradients(zip(gradients, variables))\n","\n","  # Return a dict mapping metric names to current value\n","    return {'batch_loss': average_loss}\n","\n","\n","  def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n","    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n","    \n","    # Run the decoder one step.\n","    decoder_input = DecoderInput(new_tokens=input_token,\n","                               enc_output=enc_output,\n","                               mask=input_mask)\n","\n","    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n","    \n","\n","    #self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n","    #self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n","    #self.shape_checker(dec_state, ('batch', 'dec_units'))\n","\n","  # `self.loss` returns the total for non-padded tokens\n","    y = target_token\n","    y_pred = dec_result.logits\n","    step_loss = self.loss(y, y_pred)\n","\n","    return step_loss, dec_state\n","  \n","  @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.int32, shape=[None,]),\n","                               tf.TensorSpec(dtype=tf.int32, shape=[None,])]])\n","  def _tf_train_step(self, inputs):\n","    return self._train_step(inputs)"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"jx4f6hz9SQlV","executionInfo":{"status":"ok","timestamp":1631447959169,"user_tz":-330,"elapsed":584,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["embedding_dim, units = 64, 1024\n","translator = TrainTranslator(\n","    embedding_dim, units,\n","    input_text_vocab = tokenizer_hi.word_index,\n","    output_text_vocab = tokenizer_en.word_index,\n","    use_tf_function=False)\n","\n","# Configure the loss and optimizer\n","translator.compile(\n","    optimizer=tf.optimizers.Adam(),\n","    loss=MaskedLoss(),\n",")\n"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"msSJ4rNkSQoQ"},"source":["for n in range(1):\n","  print(translator(x[:64]))\n","print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k1Bkxl2hm-ff"},"source":["# check save fuctionality\n","translator.save_weights(filepath=\"/content/v1weights.h5\")\n","x = translator(x[:2])\n","translator.load_weights(\"/content/v1weights.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xIYZaNVTvwDw","executionInfo":{"status":"ok","timestamp":1631429088515,"user_tz":-330,"elapsed":5603,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":[""],"execution_count":84,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UcPysAErc6n4","executionInfo":{"status":"ok","timestamp":1631260764525,"user_tz":-330,"elapsed":377,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"d716179b-559b-4d94-8abb-ae774d95ed71"},"source":["len(hi[0]), len(en[0])"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(64, 72)"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"ZflY3uZp14KK","executionInfo":{"status":"ok","timestamp":1631431548234,"user_tz":-330,"elapsed":370,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["embedding_dim =64\n","units =1024"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"cz2j5knad4tG","executionInfo":{"status":"ok","timestamp":1631435041068,"user_tz":-330,"elapsed":382,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["\n","\n","with strategy.scope():\n","  translator = TrainTranslator(\n","    embedding_dim, units,\n","    input_text_vocab = tokenizer_hi.word_index,\n","    output_text_vocab = tokenizer_en.word_index,\n","    use_tf_function=False)\n","\n","# Configure the loss and optimizer\n","  translator.compile(\n","    optimizer=tf.optimizers.Adam(),\n","    loss=MaskedLoss(),\n","  )"],"execution_count":62,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FUaPpyuCx1gQ"},"source":[""]},{"cell_type":"code","metadata":{"id":"ce6v4RF7BVQ6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_xH9lw1xkS_R","executionInfo":{"status":"ok","timestamp":1631436707044,"user_tz":-330,"elapsed":1608077,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"2e9a2e83-e729-4a6d-9f9d-c4b01f2cabfc"},"source":["training = translator.fit(x=x, y=y, batch_size=128, \n","                     epochs=25,  shuffle=True, verbose=1)"],"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","175/175 [==============================] - 718s 1s/step - batch_loss: 3.0559\n","Epoch 2/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 1.9168\n","Epoch 3/25\n","175/175 [==============================] - 37s 211ms/step - batch_loss: 1.5095\n","Epoch 4/25\n","175/175 [==============================] - 37s 211ms/step - batch_loss: 0.5215\n","Epoch 5/25\n","175/175 [==============================] - 37s 211ms/step - batch_loss: 0.3153\n","Epoch 6/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.2688\n","Epoch 7/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.2470\n","Epoch 8/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.2313\n","Epoch 9/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.2186\n","Epoch 10/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1965\n","Epoch 11/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1943\n","Epoch 12/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1717\n","Epoch 13/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1613\n","Epoch 14/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1479\n","Epoch 15/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1404\n","Epoch 16/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1345\n","Epoch 17/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1195\n","Epoch 18/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1077\n","Epoch 19/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.1023\n","Epoch 20/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.0887\n","Epoch 21/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.0840\n","Epoch 22/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.0721\n","Epoch 23/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.0621\n","Epoch 24/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.0698\n","Epoch 25/25\n","175/175 [==============================] - 37s 212ms/step - batch_loss: 0.0551\n"]}]},{"cell_type":"code","metadata":{"id":"DsRuoMsaWXqb","executionInfo":{"status":"ok","timestamp":1631433843200,"user_tz":-330,"elapsed":2,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["translator = training.model"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"lo7dabh79njx"},"source":["training_2 = translator.fit(x=x, y=y, batch_size=128, \n","                     epochs=10,  shuffle=True, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TQkf2dpPd0am","executionInfo":{"status":"ok","timestamp":1631437571818,"user_tz":-330,"elapsed":2156,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["pth_model = \"/content/drive/MyDrive/Dataset/Dakshina/hi/tensorflow/sent_mod_2\"\n","translator_1 = training.model\n","save_model(translator_1,pth_model)"],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"id":"BiAFqPiWBjbJ","executionInfo":{"status":"ok","timestamp":1631434411011,"user_tz":-330,"elapsed":377,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["translator_2 = training_2.model"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lsOoldrK7R2T","executionInfo":{"status":"ok","timestamp":1631438345603,"user_tz":-330,"elapsed":5,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"53d4a742-d80f-4505-ff4d-fa89fd02c4a3"},"source":["y[1]"],"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([33,  5,  4,  2,  2, 18, 14, 21,  2,  2, 21,  5,  3,  9, 14,  6,  2,\n","        2,  8,  4, 34,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","        0,  0,  0,  0,  0], dtype=int32)"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","metadata":{"id":"JkwOD9m_CC9y","executionInfo":{"status":"ok","timestamp":1631448077037,"user_tz":-330,"elapsed":500,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["pth_modell = \"/content/drive/MyDrive/Dataset/Dakshina/hi/tensorflow/sent_mod_1/weights.h5\""],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"md90sq3J3c3J","executionInfo":{"status":"ok","timestamp":1631441828671,"user_tz":-330,"elapsed":9846,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"2f516b7c-2c86-4ee9-9415-cc47b2c515dc"},"source":["print(tokenizer_en.sequences_to_texts(tf.argmax(translator_1(x[103:104]),axis=-1).numpy()))\n","print(tokenizer_en.sequences_to_texts(y[101:104]))"],"execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["['y a h a n     p a r y a a p t     m a t r a a     m e i n <eos> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>']\n","['<sos> y a h a n     p a r y a a p t     m a t r a a     m e i n <eos> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>']\n"]}]},{"cell_type":"code","metadata":{"id":"OQDdgrbq2bZe","executionInfo":{"status":"ok","timestamp":1631435016474,"user_tz":-330,"elapsed":16051,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["translator_2(x[0:1])\n","translator_2.load_weights(pth_modell)"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":132},"id":"KR23MWJe4yP6","executionInfo":{"status":"error","timestamp":1631445678523,"user_tz":-330,"elapsed":487,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"a1a573bf-405d-4852-baf0-6fe707e09bc1"},"source":["x = [[2]]\n"],"execution_count":8,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-6463637e8225>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print(assert x== int)\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"yPBCYixmmEfl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U8wnVjRBmEjO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2YGaMIcpmEmV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uRtMXk8GZUN1","executionInfo":{"status":"ok","timestamp":1631433344479,"user_tz":-330,"elapsed":390,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["class Translator(tf.Module):\n","\n","  def __init__(self, encoder, decoder,input_tokenizer,output_tokenizer):\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.input_tokenizer = input_tokenizer\n","    self.output_tokenizer = output_tokenizer\n","    #self.output_token_string_from_index = output_tokenizer.sequences_to_texts()\n","    #self.output_token_string_from_index = (\n","     #   tf.keras.layers.experimental.preprocessing.Char(\n","      #      vocabulary=output_text_vocab,\n","       #     mask_token='',\n","       #     invert=True))\n","\n","    # The output should never generate padding, unknown, or start.\n","    #index_from_string = tf.keras.layers.experimental.preprocessing.StringLookup(\n","       # vocabulary=output_tokenizer.word_index, mask_token='')\n","    token_mask_ids = 0\n","\n","    token_mask = np.zeros([len(output_tokenizer.word_index)+1], dtype=np.bool)\n","    token_mask[np.array(token_mask_ids)] = True\n","    self.token_mask = token_mask\n","\n","    self.start_token = output_tokenizer.word_index['<sos>']\n","    self.end_token = output_tokenizer.word_index['<eos>']"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"d2cRSfqpZUQg","executionInfo":{"status":"ok","timestamp":1631433361552,"user_tz":-330,"elapsed":386,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["def sample(self, logits, temperature):\n","  #shape_checker = ShapeChecker()\n","  # 't' is usually 1 here.\n","  #shape_checker(logits, ('batch', 't', 'vocab'))\n","  #shape_checker(self.token_mask, ('vocab',))\n","  #logits =tf.squeeze(logits,axis=1)\n","  \n","  token_mask = self.token_mask[tf.newaxis,tf.newaxis, :]\n","  \n","  #shape_checker(token_mask, ('batch', 't', 'vocab'), broadcast=True)\n","\n","  # Set the logits for all masked tokens to -inf, so they are never chosen.\n","  logits = tf.where(self.token_mask, -np.inf, logits)\n","\n","  if temperature == 0.0:\n","    new_tokens = tf.argmax(logits, axis=-1)\n","  else: \n","    logits = tf.squeeze(logits, axis=1)\n","    new_tokens = tf.random.categorical(logits/temperature,\n","                                        num_samples=1)\n","\n","  #shape_checker(new_tokens, ('batch', 't'))\n","\n","  return new_tokens\n","def tokens_to_text(self, result_tokens):\n","  #shape_checker = ShapeChecker()\n","  #shape_checker(result_tokens, ('batch', 't'))\n","  result_text_tokens = self.output_tokenizer.sequences_to_texts(result_tokens.numpy())\n","  #shape_checker(result_text_tokens, ('batch', 't'))\n","\n","  #result_text = tf.strings.reduce_join(result_text_tokens,\n","   #                                    axis=1, separator='')\n","  #shape_checker(result_text, ('batch'))\n","\n","  #result_text = tf.strings.strip(result_text)\n","  #shape_checker(result_text, ('batch',))\n","  return result_text_tokens\n","def translate_unrolled(self,\n","                       input_text, *,\n","                       max_length=50,\n","                       return_attention=False,\n","                       temperature=0.0):\n","  \n","  #print(batch_size)\n","  input_tokens = self.input_tokenizer.texts_to_sequences(input_text)\n","  input_tokens = kprocessing.sequence.pad_sequences(input_tokens,padding='post')\n","  #print(input_tokens.T,input_tokens.T.shape)\n","  input_tokens= input_tokens.T\n","  batch_size = input_tokens.shape[0]\n","  input_tokens = tf.constant(input_tokens)\n","  enc_output, enc_state = self.encoder(input_tokens)\n","\n","  dec_state = enc_state\n","  new_tokens = tf.fill([batch_size, 1], self.start_token)\n","  #print(new_tokens)\n","  result_tokens = []\n","  attention = []\n","  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n","  \n","  #print(max_length)\n","  for _ in range(max_length):\n","    dec_input = DecoderInput(new_tokens=new_tokens,\n","                             enc_output=enc_output,\n","                             mask=(input_tokens!=0))\n","\n","    dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n","    #print(dec_result.logits.shape)\n","    attention.append(dec_result.attention_weights)\n","\n","    new_tokens = self.sample(dec_result.logits, temperature)\n","\n","    # If a sequence produces an `end_token`, set it `done`\n","    done = done | (new_tokens == self.end_token)\n","    # Once a sequence is done it only produces 0-padding.\n","    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n","\n","    # Collect the generated tokens\n","    result_tokens.append(new_tokens)\n","\n","    if tf.executing_eagerly() and tf.reduce_all(done):\n","      break\n","\n","  # Convert the list of generates token ids to a list of strings.\n","  \n","  result_tokens = tf.concat(result_tokens, axis=-1)\n","  result_text = self.tokens_to_text(result_tokens)\n","\n","  if return_attention:\n","    attention_stack = tf.concat(attention, axis=1)\n","    return {'text': result_text, 'attention': attention_stack}\n","  else:\n","    return {'text': result_text}\n","\n","\n","Translator.tokens_to_text = tokens_to_text\n","Translator.sample = sample\n","Translator.translate = translate_unrolled"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"KWDWnR4MNddD","executionInfo":{"status":"ok","timestamp":1631441537595,"user_tz":-330,"elapsed":383,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["translator = Translator(\n","    encoder=translator_1.encoder,\n","    decoder=translator_1.decoder,\n","    input_tokenizer=tokenizer_hi,\n","    output_tokenizer=tokenizer_en,\n",")"],"execution_count":77,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":436},"id":"apgF-6qwNhnt","executionInfo":{"status":"error","timestamp":1631441563015,"user_tz":-330,"elapsed":24112,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}},"outputId":"107ce9f4-b2d8-45f7-8023-a1687b7dbbd5"},"source":["ns = [i for i in range(100,200)]\n","for i in ns:\n","  print(translator.translate(hi[i]), en[i])"],"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["{'text': ['y a h a n     p a r y a p t     m a t r a     m e n s h o s i s t     m e n <UNK>']} yahan  paryaapt  matraa  mein<eos>\n","{'text': ['h o s i m e n t     k a     n i r m a a n     h o t a a s i s s a y     h o t a a s i n s a y     h']} siment  ka  nirmaan  hota<eos>\n","{'text': ['h a h a i   .   h a h a i   . <UNK>']} hai.<eos>\n","{'text': ['y a h     a p n e     s a m a y     k i e s h a y     k i e <UNK>']} yah  apne  samay  ki<eos>\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-78-e7f65bdea761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-47-ec4ad3cdc77f>\u001b[0m in \u001b[0;36mtranslate_unrolled\u001b[0;34m(self, input_text, max_length, return_attention, temperature)\u001b[0m\n\u001b[1;32m     64\u001b[0m                              mask=(input_tokens!=0))\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mdec_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdec_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;31m#print(dec_result.logits.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-33-d1fcc216b5b9>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mattention_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_and_rnn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m  \u001b[0;31m# shape_checker(attention_vector, ('batch', 't', 'dec_units'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m         training=training_mode):\n\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m       \u001b[0minput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0meager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0mcall_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;31m# guarding for is a Layer instance (Functional API), which does not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;31m# have a `shape` attribute.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Inputs to a layer should be tensors. Got: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;31m# `_tensor_shape` is declared and defined in the definition of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;31m# `EagerTensor`, in C.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"6YA5WZ2GCVvb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K2Y_rBCEDTf6","executionInfo":{"status":"ok","timestamp":1631431104059,"user_tz":-330,"elapsed":416,"user":{"displayName":"ShuvraNeel Roy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjANYVmLo_zt-mrVqbJH5tqTlykNEjoP2JtAN8Ybw=s64","userId":"06350349201381820342"}}},"source":["en = df['roman']\n","hi = df['Hindi']\n","def shorten(hi,en,max_words):\n","  new_hi = []\n","  new_en = []\n","  for i,j in zip(hi,en):\n","    hi_wor = i.split(\" \")\n","    en_wor = j.split(\" \")\n","    l=4\n","    for k in range(0,len(hi_wor)):\n","      \n","      if (k%max_words == 0) :\n","        new_hi.append(\"  \".join(hi_wor[k:k+l])+ \"<eos>\")\n","        new_en.append(\"  \".join(en_wor[k:k+l])+ \"<eos>\")\n","        l=0\n","      #elif k == len(hi_wor)-1:\n","      #  new_hi.append(hi_wor[k-l:])\n","      l+=1\n","    \n","  return new_hi,new_en\n","hi,en = shorten(hi,en,4)"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"KQrMcyI8Dzre"},"source":[""],"execution_count":null,"outputs":[]}]}